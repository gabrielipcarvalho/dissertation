->>> This is what code I have written so far:
{{{
# File: news_fetch.py

import requests
import time
import json
from pprint import pprint/Users/gabriel/Documents/github/dissertation/scheme5.txt
/Users/gabriel/Documents/github/dissertation/scheme4.txt
/Users/gabriel/Documents/github/dissertation/scheme3.txt
/Users/gabriel/Documents/github/dissertation/scheme2.txt
from datetime import datetime, timedelta
import os

# Replace with your credentials
username = 'gabrielisaias.paduacarvalho@griffithuni.edu.au'
password = 'Albusnox1@'
AppID = 'e77ae954'


def get_auth_header(username, password, appid):
    # Generate the authorization header for making requests to the Aylien API.
    token_response = requests.post(
        'https://api.aylien.com/v1/oauth/token',
        auth=(username, password),
        data={'grant_type': 'password'}
    )
    token_response.raise_for_status()
    token = token_response.json()['access_token']
    headers = {
        'Authorization': f'Bearer {token}',
        'AppId': appid
    }
    return headers


def get_stories(params, headers, max_stories=100):
    # Fetch stories from the Aylien News API using the provided parameters and headers.
    fetched_stories = []
    stories = None

    while stories is None or len(stories) > 0:
        try:
            response = requests.get('https://api.aylien.com/v6/news/stories', params=params, headers=headers,
                                    timeout=30)
            if response.status_code == 200:
                response_json = response.json()
                stories = response_json.get('stories', [])
                fetched_stories.extend(stories)
                if len(fetched_stories) >= max_stories:
                    fetched_stories = fetched_stories[:max_stories]
                    break
                if 'next_page_cursor' in response_json:
                    params['cursor'] = response_json['next_page_cursor']
                else:
                    break
                print(f"Fetched {len(stories)} stories. Total story count so far: {len(fetched_stories)}")
            elif response.status_code == 429:
                print("Rate limit reached. Sleeping for 10 seconds.")
                time.sleep(10)
            elif 500 <= response.status_code <= 599:
                print(f"Server error {response.status_code}. Sleeping for 260 seconds.")
                time.sleep(260)
            else:
                pprint(response.text)
                break
        except requests.exceptions.Timeout:
            print("Request timed out. Retrying...")
            continue
        except Exception as e:
            print(e)
            break
    return fetched_stories


def filter_stories(stories):
    filtered_stories = []
    for story in stories:
        # Keep the basic fields
        filtered_story = {
            "author": story.get("author", {}),
            "body": story.get("body", ""),
            "summary": story.get("summary", {}).get("sentences", []),
            "title": story.get("title", ""),
            "source": {
                "domain": story.get("source", {}).get("domain", ""),
                "home_page_url": story.get("source", {}).get("home_page_url", ""),
                "name": story.get("source", {}).get("name", "")
            }
        }

        # Filter the categories
        filtered_categories = [
            {"id": category.get("id", ""), "label": category.get("label", ""), "score": category.get("score", "")}
            for category in story.get("categories", [])
            if category.get("id") == "ay.impact"
        ]

        # Add categories if the filtered list is not empty
        if filtered_categories:
            filtered_story["categories"] = filtered_categories

        filtered_stories.append(filtered_story)

    return filtered_stories


def fetch_and_save_news_for_day(date, counter):
    headers = get_auth_header(username, password, AppID)
    params = {
        'published_at': f'[{date}T00:00:00Z TO {date}T23:59:59Z]',
        'language': 'en',
        'categories': '{{taxonomy:aylien AND id:(ay.biz.dividend OR ay.lifesoc.disater OR ay.fin.sharehld OR ay.fin.reports OR ay.pol.civilun OR ay.biz.regulat OR ay.impact.ops OR ay.impact.ratings OR ay.biz.bankrupt) AND score: [0.8 TO 1]}}',
        'source.name': '("Yahoo Finance" OR "Marketwatch" OR "Investing" OR "Nasdaq" OR "CNBC" OR "StockMarketWire (UK)" OR "Market Screener" OR "Seeking Alpha" OR "Investors.com" OR "The Motley Fool" OR "INO" OR "Money Control" OR "AlphaStreet" OR "Equitymaster" OR "Washingtonpost.com" OR "New York Times, The" OR "Wall Street Journal")',
        'sentiment.title.polarity': '(negative OR neutral OR positive)',
        'sort_by': 'relevance',
        'per_page': 100
    }

    response = requests.get('https://api.aylien.com/v6/news/stories', params=params, headers=headers)
    response.raise_for_status()
    response_json = response.json()

    stories = response_json.get('stories', [])
    filtered_stories = filter_stories(stories)

    # Ensure the directory exists
    os.makedirs('./News_Data', exist_ok=True)

    # Save the filtered stories to a JSON file
    if filtered_stories:
        filename = f"./News_Data/{counter}_{date.replace('-', '_')}.json"
        with open(filename, "w") as file:
            json.dump(filtered_stories, file, indent=4)
        print(f"Filtered stories saved to {filename}")
    else:
        print(f"No stories found for {date}")


if __name__ == '__main__':
    start_date = datetime.strptime("2023-01-02", "%Y-%m-%d")
    end_date = datetime.strptime("2024-07-14", "%Y-%m-%d")
    current_date = start_date
    counter = 1

    while current_date <= end_date:
        date_str = current_date.strftime("%Y-%m-%d")
        fetch_and_save_news_for_day(date_str, counter)
        current_date += timedelta(days=1)
        counter += 1

// File: GPTA_Adaptor.js

// Import necessary libraries
const { OpenAI } = require("openai");
const fs = require("fs").promises; // Using fs.promises for async file operations
require("dotenv").config();

// Configuration using GPTA's specific API key from the .env file
const openai = new OpenAI({
	apiKey: process.env.GPTA_API_KEY,
});

// Utility function to log data to a file, ensuring it is in JSON format
const logToFile = async (filename, data) => {
	const path = `Data/${filename}`;
	const dataToLog = JSON.stringify(data, null, 2); // Convert data to a JSON string
	await fs.writeFile(path, dataToLog, { encoding: "utf8" }); // Write data as JSON
};

// Function to collect and log news data from a file
const collectAndLogNews = async (day) => {
	const filePath = `Data/news-data-${day}.txt`;

	try {
		const newsDataText = await fs.readFile(filePath, {
			encoding: "utf8",
		});

		// Log the collected news data as JSON
		await logToFile(`log.news.${day}.txt`, { content: newsDataText });

		return newsDataText; // Return text directly
	} catch (error) {
		console.error(`Error reading news data from file for ${day}:`, error);
		throw error;
	}
};

// Function to extract key information from news using GPT-4 with the chat completions endpoint
const extractKeyInformation = async (newsData, day) => {
	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"Please analyse the following text and extract key information that could significantly impact stock market trends. Focus on identifying critical details related to corporate earnings, economic announcements, geopolitical events, market forecasts, and other influential factors. Ensure the summary is concise and highlights the potential market implications of each identified element.",
				},
				{
					role: "user",
					content: newsData,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			console.error(
				"Invalid response from the API or missing data:",
				completion
			);
			throw new Error("Invalid response structure from API.");
		}

		const extractedInformation =
			completion.choices[0].message.content.trim();
		await logToFile(`log.GPTA.${day}.extracted.txt`, {
			extractedInformation,
		});

		return extractedInformation;
	} catch (error) {
		console.error(`Error extracting key information for ${day}:`, error);
		throw error;
	}
};

// Function to execute sentiment analysis on extracted news, now using the chat completions endpoint
const executeSentimentAnalysis = async (extractedInformation, day) => {
	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"Utilise the provided information to perform a comprehensive sentiment analysis, determining the overall sentiment (positive, negative, or neutral) and its intensity. Focus on how this sentiment might affect stock market trends, considering the potential impact on market movements, investor behaviour, and future market forecasts. Provide a detailed explanation of your analysis and its implications for stock market trends.",
				},
				{
					role: "user",
					content: extractedInformation,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			console.error(
				"Invalid response from the API or missing data:",
				completion
			);
			throw new Error("Invalid response structure from API.");
		}

		const sentimentAnalysis = completion.choices[0].message.content.trim();
		await logToFile(`log.GPTA.${day}.sentiment.txt`, { sentimentAnalysis });

		return sentimentAnalysis;
	} catch (error) {
		console.error(`Error during sentiment analysis for ${day}:`, error);
		throw error;
	}
};

// Export functions to be used by the orchestration program
module.exports = {
	collectAndLogNews,
	extractKeyInformation,
	executeSentimentAnalysis,
};


// File: GPTB_Adaptor.js

// Import necessary libraries
const { OpenAI } = require("openai");
const fs = require("fs").promises;
require("dotenv").config();

// Configuration using GPTB's specific API key from the .env file
const openai = new OpenAI({
	apiKey: process.env.GPTB_API_KEY,
});

// Utility function to read and validate JSON data logged by GPTA
const readAndValidateData = async (filename) => {
	const filePath = `Data/${filename}`;
	try {
		const dataJson = await fs.readFile(filePath, { encoding: "utf8" });
		const data = JSON.parse(dataJson);
		if (!data) {
			throw new Error("Invalid or empty JSON data");
		}
		return data;
	} catch (error) {
		console.error(`Error processing file: ${filePath}`, error);
		throw error; // Ensure errors are propagated up
	}
};

// Function to read and parse stock price data which is not in JSON format
const readStockPriceData = async (filename) => {
	const filePath = `Data/${filename}`;
	try {
		const stockPriceDataText = await fs.readFile(filePath, {
			encoding: "utf8",
		});
		// Assuming the data is tab-separated; adjust if it uses commas or another delimiter
		const rows = stockPriceDataText
			.split("\n")
			.map((row) => row.split("\t"));

		// Optionally convert rows into a more structured format if needed
		const headers = rows[0];
		const data = rows.slice(1).map((row) => {
			let rowData = {};
			row.forEach((value, index) => {
				rowData[headers[index]] = value;
			});
			return rowData;
		});

		return data; // Return parsed data as an array of objects
	} catch (error) {
		console.error(`Error processing stock price file: ${filePath}`, error);
		throw error;
	}
};

// Function to analyze how news, sentiment, and stock prices affected market trends
const analyzeImpactOnStockPrices = async (
	extractedInfo,
	sentimentAnalysis,
	stockPrices,
	day
) => {
	const prompt = `Utilise the following data to conduct a comprehensive analysis of how these factors might influence stock price movements for ${day}. Focus on the extracted information, its sentiment analysis, and the corresponding stock price data. Your analysis should cover the following aspects in detail:

1. **Relevance to Stock Prices**: Identify and explain the direct relevance of the extracted information to stock market trends. Specify the presence of company earnings reports, policy changes, market sentiment shifts, geopolitical events, or other significant factors, and discuss their potential impact on stock prices.

2. **Sentiment Influence**: Analyse how the sentiment (positive, negative, neutral) expressed in the news correlates with observed or potential stock price movements. Consider whether the sentiment could lead to increased trading volume, market optimism or pessimism, or price volatility, and provide examples to support your analysis.

3. **Causative Links**: Establish and explain any causative links between the news sentiment and stock price fluctuations. Highlight clear patterns where certain types of news or sentiment consistently impact stock prices, and discuss the strength and reliability of these patterns.

4. **Comparative Analysis**: Compare the impact of the current day's news and sentiment with data from previous days. Identify cumulative effects or changing trends that might influence future stock price predictions. Discuss how these trends could affect market behaviour and provide insights for future analysis.

5. **Potential Anomalies or Exceptions**: Identify and explain any anomalies or exceptions where the expected impact of news sentiment did not align with actual stock price movements. Suggest possible reasons for these discrepancies, considering factors such as market conditions, external influences, or data inconsistencies.

Provide a detailed and structured analysis, incorporating quantitative and qualitative insights to support your conclusions.`;

	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{ role: "system", content: prompt },
				{
					role: "user",
					content: JSON.stringify({
						extractedInformation: extractedInfo,
						sentiment: sentimentAnalysis,
						stockPrices: stockPrices,
					}),
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			throw new Error("Invalid response structure from API.");
		}

		const impactAnalysis = completion.choices[0].message.content.trim();
		await fs.writeFile(
			`Data/log.GPTB.${day}.impact.json`,
			JSON.stringify({ impactAnalysis }, null, 2),
			{ encoding: "utf8" }
		);
		return impactAnalysis;
	} catch (error) {
		console.error(
			`Error analyzing impact on stock prices for ${day}:`,
			error
		);
		throw error;
	}
};

// Function to predict future stock prices based on the analysis
const predictStockPrices = async (impactAnalysis, day) => {
	const prompt = `Using the analysis of news impact and market sentiment from Day ${
		parseInt(day.replace("day", "")) - 1
	}, forecast the stock prices for Day ${parseInt(
		day.replace("day", "")
	)}. Your prediction should clearly state whether stock prices will rise or fall, by how much, and the reasoning behind your forecast. Ensure that the prediction is quantitative, specifying the expected percentage change or price range. Consider all relevant factors such as market trends, sentiment shifts, historical data, and any anomalies observed. The prediction must be actionable and precise, enabling further validation and fine-tuning.`;

	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{ role: "system", content: prompt },
				{
					role: "user",
					content: impactAnalysis,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			throw new Error("Invalid response structure from API.");
		}

		const prediction = completion.choices[0].message.content.trim();
		await fs.writeFile(
			`Data/log.GPTB.prediction.day${parseInt(
				day.replace("day", "")
			)}.json`,
			JSON.stringify({ prediction }, null, 2),
			{ encoding: "utf8" }
		);
		return prediction;
	} catch (error) {
		console.error(`Error predicting stock prices for Day ${day}:`, error);
		throw error;
	}
};

// Export functions to be used by the orchestration program
module.exports = {
	readAndValidateData,
	readStockPriceData,
	analyzeImpactOnStockPrices,
	predictStockPrices,
};


// File: GPTC_Adaptor.js
const { OpenAI } = require("openai");
const fs = require("fs").promises;

// Configuration using GPTC's specific API key from the .env file
const openai = new OpenAI({
	apiKey: process.env.GPTC_API_KEY,
});

// Function to read and parse stock price data from a TSV file
const readStockPriceData = async (filename) => {
	const filePath = `Data/${filename}`;
	try {
		const stockPriceDataText = await fs.readFile(filePath, {
			encoding: "utf8",
		});
		const rows = stockPriceDataText
			.trim()
			.split("\n")
			.map((row) => row.split("\t"));
		const headers = rows.shift(); // Extract headers
		const data = rows.map((row) => {
			return row.reduce((obj, value, index) => {
				obj[headers[index]] = value;
				return obj;
			}, {});
		});
		return data;
	} catch (error) {
		console.error(
			`Error reading stock price data from file: ${filePath}`,
			error
		);
		throw error;
	}
};

// Function to analyze stock prices using GPT model
const analyzeStockPricesWithGPT = async (stockPrices) => {
	const prompt = `Analyze the following stock price data for trends and patterns, and make a concrete prediction for the next trading day. Clearly state whether stock prices are expected to rise or fall, and specify the expected percentage change or price range. Your prediction must be quantitative and actionable, enabling validation against actual market outcomes. Consider historical trends, recent market behaviour, and any notable anomalies in the data. The stock price data to analyze is: ${JSON.stringify(
		stockPrices
	)}`;

	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"You are a financial analyst. Your task is to analyze the provided stock price data and make a concrete prediction about future stock price movements. Your prediction must be clear, quantitative, and actionable.",
				},
				{ role: "user", content: prompt },
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			console.error(
				"Invalid response from the API or missing data:",
				completion
			);
			throw new Error("Invalid response structure from API.");
		}

		const insights = completion.choices[0].message.content.trim();
		return insights;
	} catch (error) {
		console.error(
			"Error during GPT model analysis of stock prices:",
			error
		);
		throw error;
	}
};

// Export functions to be used by the orchestration program
module.exports = {
	readStockPriceData,
	analyzeStockPricesWithGPT,
};


// File: GPTD_Adaptor.js
const { OpenAI } = require("openai");
const fs = require("fs").promises;
require("dotenv").config();

// Configuration using GPTD's specific API key from the .env file
const openai = new OpenAI({
	apiKey: process.env.GPTD_API_KEY,
});

// Function to read JSON data from a file
const readJSONData = async (filename) => {
	const filePath = `Data/${filename}`;
	try {
		const dataJson = await fs.readFile(filePath, { encoding: "utf8" });
		return JSON.parse(dataJson);
	} catch (error) {
		console.error(`Error reading JSON file: ${filePath}`, error);
		throw error;
	}
};

// Function to integrate and analyze predictions from GPTB and GPTC
const integrateAndAnalyzePredictions = async (day) => {
	try {
		const nextDay = `day${parseInt(day.replace("day", "")) + 1}`;
		const gptbPredictionData = await readJSONData(
			`log.GPTB.prediction.${nextDay}.json`
		);
		const gptcAnalysisData = await readJSONData(
			`log.GPTC.${day}.analysis.json`
		);

		const prompt = `Integrate and analyze predictions from GPTB and GPTC for Day ${day}, assessing the alignment and discrepancies between the two forecasts. Ensure the analysis highlights key points of agreement and divergence between the models, providing a comprehensive understanding of their predictions. Predictions from GPTB: ${JSON.stringify(
			gptbPredictionData.prediction
		)}, Predictions from GPTC: ${JSON.stringify(gptcAnalysisData)}.`;

		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"Synthesize the information from GPTB and GPTC models to provide a cohesive analysis. Your analysis should integrate insights from both models, highlighting areas of agreement and divergence, and explain the implications for stock price movements. Ensure the analysis is detailed and includes quantitative assessments where possible.",
				},
				{
					role: "user",
					content: prompt,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			throw new Error("Invalid response structure from API.");
		}

		const combinedAnalysis = completion.choices[0].message.content.trim();
		await fs.writeFile(
			`Data/log.GPTD.${day}.analysis.json`,
			JSON.stringify({ combinedAnalysis }, null, 2),
			{ encoding: "utf8" }
		);

		return combinedAnalysis;
	} catch (error) {
		console.error(
			"Error during integration and analysis of predictions:",
			error
		);
		throw error;
	}
};

// Function to make a final prediction for the next trading day stock prices
const makeFinalPrediction = async (day) => {
	try {
		const analysisData = await readJSONData(
			`log.GPTD.${day}.analysis.json`
		);

		const nextDay = `day${parseInt(day.replace("day", "")) + 1}`;
		const prompt = `Based on the integrated analysis from Day ${day}, synthesize insights to make a final, comprehensive prediction for ${nextDay} stock prices. Your prediction should clearly state whether stock prices are expected to rise or fall, by how much, and the reasoning behind your forecast. Ensure the prediction is quantitative, specifying the expected percentage change or price range. Consider historical trends, recent market behaviour, and any notable anomalies in the data. Analysis data: ${JSON.stringify(
			analysisData
		)}.`;

		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"Provide a detailed forecast using the integrated analysis from GPTB and GPTC. Your forecast should clearly state whether stock prices will rise or fall, by how much, and include the reasoning behind your prediction. Ensure the forecast is actionable and precise, enabling validation against actual market outcomes.",
				},
				{
					role: "user",
					content: prompt,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			throw new Error("Invalid response structure from API.");
		}

		const finalPrediction = completion.choices[0].message.content.trim();
		await fs.writeFile(
			`Data/log.GPTD.${nextDay}.prediction.json`,
			JSON.stringify({ finalPrediction }, null, 2),
			{ encoding: "utf8" }
		);

		return finalPrediction;
	} catch (error) {
		console.error("Error making final prediction for stock prices:", error);
		throw error;
	}
};

// Export functions to be used by the orchestration program
module.exports = {
	integrateAndAnalyzePredictions,
	makeFinalPrediction,
};


// File: orchestration_program.js

// Import necessary libraries and adaptors
const {
	collectAndLogNews,
	extractKeyInformation,
	executeSentimentAnalysis,
} = require("./GPTA_Adaptor");
const {
	readAndValidateData,
	readStockPriceData: readBStockPriceData,
	analyzeImpactOnStockPrices,
	predictStockPrices,
} = require("./GPTB_Adaptor");
const {
	readStockPriceData: readCStockPriceData,
	analyzeStockPricesWithGPT,
} = require("./GPTC_Adaptor");
const {
	integrateAndAnalyzePredictions,
	makeFinalPrediction,
} = require("./GPTD_Adaptor");
const fs = require("fs").promises;
require("dotenv").config();

// Orchestration function to coordinate adaptors
const orchestrateAdaptors = async (day) => {
	try {
		// Step 1: GPTA collects and processes news data
		const newsData = await collectAndLogNews(day);
		const extractedInfo = await extractKeyInformation(newsData, day);
		const sentimentAnalysis = await executeSentimentAnalysis(
			extractedInfo,
			day
		);

		// Log outputs from GPTA for verification and further use
		const logDataGPTA = {
			extractedInformation: extractedInfo,
			sentimentAnalysis: sentimentAnalysis,
		};
		await fs.writeFile(
			`Data/log.GPTA.${day}.output.json`,
			JSON.stringify(logDataGPTA, null, 2),
			{ encoding: "utf8" }
		);

		// Step 2: GPTB reads the data validated by GPTA and analyses impact
		const extractedInfoData = await readAndValidateData(
			`log.GPTA.${day}.output.json`
		);
		const stockPricesDataB = await readBStockPriceData(
			`prices-data-${day}.txt`
		);

		// Analyze the impact on stock prices using GPTB
		const impactAnalysisB = await analyzeImpactOnStockPrices(
			extractedInfoData.extractedInformation,
			extractedInfoData.sentimentAnalysis,
			stockPricesDataB,
			day
		);
		await fs.writeFile(
			`Data/log.GPTB.${day}.impact.json`,
			JSON.stringify({ impactAnalysis: impactAnalysisB }, null, 2),
			{ encoding: "utf8" }
		);

		// Step 3: GPTB makes a prediction based on its analysis
		const predictionDay = `day${parseInt(day.replace("day", "")) + 1}`;
		const predictionB = await predictStockPrices(
			impactAnalysisB,
			predictionDay
		);
		await fs.writeFile(
			`Data/log.GPTB.prediction.${predictionDay}.json`,
			JSON.stringify({ prediction: predictionB }, null, 2),
			{ encoding: "utf8" }
		);

		// Step 4: GPTC reads and analyses the raw stock price data
		const stockPricesDataC = await readCStockPriceData(
			`prices-data-${day}.txt`
		);
		const impactAnalysisC = await analyzeStockPricesWithGPT(
			stockPricesDataC
		);
		await fs.writeFile(
			`Data/log.GPTC.${day}.analysis.json`,
			JSON.stringify({ impactAnalysis: impactAnalysisC }, null, 2),
			{ encoding: "utf8" }
		);

		// Step 5: GPTD integrates and analyzes predictions from GPTB and GPTC, then makes a final prediction
		const integratedAnalysis = await integrateAndAnalyzePredictions(day);
		const finalPrediction = await makeFinalPrediction(day);
		await fs.writeFile(
			`Data/log.GPTD.${predictionDay}.prediction.json`,
			JSON.stringify({ finalPrediction }, null, 2),
			{ encoding: "utf8" }
		);

		console.log(`Orchestration completed for ${day}`);
	} catch (error) {
		console.error("Error during orchestration:", error);
	}
};

// Example usage to orchestrate operations for a specific day
(async () => {
	const day = "day0"; // Specify the day for processing
	await orchestrateAdaptors(day);
})();

module.exports = { orchestrateAdaptors };

}}}

->>> This is my research proposal:
{{{
GRIFFITH UNIVERSITY
SCHOOL OF INFORMATION AND COMMUNICATION TECHNOLOGY

Student: Gabriel Isaias Padua Carvalho (Id: s5326266)
Supervisor: Dr. Larry Wen
Title: Harnessing Off-the-Shelf Transformer AI for Beyond-Label Financial Predictive Analytics: An Exploration via Node.js Orchestration

Context: This dissertation, to be submitted for the Master of IT degree at Griffith University, delves into the practicality and efficiency of applying pre-trained Transformer models to domains beyond their initial training objectives. With a view towards publication, this research scrutinises the potential broad-spectrum utility of such AI models in the financial sector, using the stock market as a test bed.

Technology Stack:
•	Programming Language: JavaScript
•	Runtime Environment: Node.js
•	AI Models: GPT, Transformer AI

Project Overview:
Objective: Central to this investigation is the determination of whether off-the-shelf pre-trained AI models, particularly GPT and other Transformer architectures, can be effectively fine-tuned for tasks significantly divergent from their original training purposes. This enquiry stems from the high costs associated with developing bespoke AI solutions for niche applications, positing pre-trained models as a potentially cost-effective alternative.
Approach: The study pioneers a dynamic orchestration system that commands multiple GPT model instances through bespoke adaptors. This innovative setup facilitates the nuanced fine-tuning and integration necessary for the models to adapt to the intricacies of financial analytics.

Key Features:
•	Robust Data Management: The orchestrated system showcases an exceptional capability for handling extensive datasets, ensuring fluid communication and data sharing across different analytical agents.
•	Bespoke Adaptors for Model Orchestration: Custom-designed adaptors empower the system with unparalleled flexibility, enabling the integration of varied GPT model instances tailored to the specific analytical demands of the financial markets.
•	Targeted Fine-tuning: Beyond general application, the project emphasises the strategic fine-tuning of Transformer AI models to refine their efficacy for predictive analytics within the financial domain.

Impact and Potential: This research underscores the significant opportunities that lie in re-purposing pre-trained, readily available AI models for specialised applications. By demonstrating the feasibility of adapting these models to the financial analytics domain through strategic fine-tuning and sophisticated data orchestration, the study not only broadens the academic understanding of AI flexibility but also opens the door to myriad practical applications in sectors where the cost of bespoke AI development is prohibitively high.

Conclusion: In examining the adaptability of pre-trained Transformer AI to tasks far removed from their original scope, particularly within the volatile environment of the stock market, this dissertation offers compelling evidence of the transformative potential of such models. It lays the groundwork for future explorations into the use of off-the-shelf AI in various industries, challenging the traditional paradigms of AI development and application.

Phase 1: Foundation and Conceptual Framework (Weeks 1-4)

Overview
The inaugural phase of this project is meticulously designed to establish a robust intellectual and technical foundation. It encompasses a comprehensive review of pertinent technologies and methodologies, alongside the initiation of a conceptual framework that will guide the project's trajectory. This phase is characterised by its focus on acquiring a deep understanding of the underlying technologies—specifically JavaScript, Node.js for the runtime environment, and the utilisation of Transformer AI models like GPT— and the initial steps towards practical application in financial predictive analytics.

Goals
1.	In-depth Understanding of Key Technologies: Achieve a thorough grasp of JavaScript and Node.js, emphasising their capabilities, limitations, and potential for integration with Transformer AI models. This involves a detailed review of the latest documentation and relevant technical materials.
2.	Literature Survey: Conduct an exhaustive literature review to understand the current landscape of Transformer models in finance, identifying gaps and opportunities where off-the-shelf AI models like GPT can be innovatively applied.
3.	Development of Conceptual Framework: Lay the groundwork for a conceptual framework that outlines the project’s approach to integrating AI models with financial analytics, including preliminary designs for data adaptors and the API server.
4.	Prototype Development: Initiate the practical application of theoretical knowledge by developing prototype data adaptors, thereby starting the transition from concept to tangible experimentation.

Tasks and Activities
•	Technical Documentation Review: Allocate the initial weeks to thoroughly review and annotate the OpenAI and Express.js documentation. This task is foundational, ensuring a clear understanding of how GPT models can be orchestrated within a Node.js environment for financial analytics.
•	Literature Review and Synthesis: Simultaneously, engage in a critical literature review focusing on the intersection of AI and financial prediction. This involves summarising key articles, extracting insights on model integration, and identifying methodologies that could influence the development of data adaptors and model integration strategies.
•	Conceptual Design: Based on insights gained from the technical review and literature survey, sketch the preliminary architecture for data adaptors and the API server. This involves outlining the data flow, model interaction endpoints, and the initial API design capable of supporting dynamic interactions with GPT models.
•	Prototype Development: Transition theoretical concepts into practice by developing prototype data adaptors. This step marks the project’s first foray into tangible development, setting the stage for later refinements and testing.

Expected Outcomes
By the end of Phase 1, the project aims to have achieved a solid understanding of the technologies and theoretical frameworks that will underpin the research. This includes:
•	A comprehensive summary of OpenAI and Express.js capabilities, tailored to the project's needs.
•	A synthesised review of existing literature, highlighting potential applications and innovations within financial analytics using AI.
•	Preliminary designs for the system architecture, including data adaptors and API server blueprints, ready for iterative development and testing in subsequent phases.
•	Initial prototype data adaptors, establishing a baseline for practical experimentation and integration with GPT models.

Summary
Phase 1 is crucial for setting a strong foundation for the project. It involves a balanced blend of theoretical study and practical exploration, ensuring that subsequent phases are informed by a deep understanding of both the technological landscape and the current academic discourse in AI applications in finance. The completion of this phase is a critical milestone, establishing the groundwork from which the project can advance towards its innovative objectives with clarity and confidence. The meticulous approach to understanding, designing, and beginning tangible development work during this phase ensures that the project is well-primed to tackle more complex challenges in the realms of AI integration, financial predictive analytics, and beyond.


Phase 2: System Development and Initial Testing (Weeks 5-12)

Overview
Phase 2 is crucial for transitioning from foundational understanding and preliminary designs to the development and testing of functional prototypes. This stage is marked by an iterative and agile approach to both software development and system integration, leveraging insights gained from the initial literature review and the foundational work of Phase 1. The primary focus here is on refining the API server and data adaptors, integrating these components with GPT model instances, and conducting initial system tests to assess functionality and data flow.

Goals
1.	Prototype Refinement and Development: Further develop and refine the prototype data adaptors and API server, incorporating findings from the extended literature survey and initial testing feedback.
2.	System Integration: Seamlessly integrate the developed components (data adaptors, API server, and GPT models) to create a coherent system capable of performing basic financial predictive analytics tasks.
3.	Preliminary Testing: Conduct initial tests to evaluate the system’s data management, model interaction efficiency, and the overall system’s resilience and reliability.
4.	Documentation and Iterative Improvement: Maintain comprehensive documentation of the development process, test outcomes, and the rationale behind design and integration decisions, facilitating continuous improvement.

Tasks and Activities
•	Extended Literature Survey: Continue the exploration of recent academic and industry developments in AI and financial analytics to gather insights that can influence further development and integration strategies.
•	Refinement of Data Adaptors and API Server: Leverage insights from the ongoing literature review to refine the design and functionality of data adaptors and the API server, focusing on efficiency, scalability, and the capability to handle complex financial datasets.
•	Integration of System Components: Methodically integrate the refined data adaptors with the API server and GPT models, ensuring a seamless flow of data and functionality across the system. This task is crucial for enabling dynamic model interactions and data exchange.
•	Initial System Testing: Design and execute a series of preliminary tests to assess the integrated system’s performance, identifying any bottlenecks or inefficiencies in data handling, model interaction, and predictive analytics capabilities.
•	Documentation and Feedback Incorporation: Systematically document the development and testing process, including challenges encountered, solutions implemented, and insights gained. This documentation serves as a foundation for iterative refinement and is essential for tracking the project's evolution.

Expected Outcomes
•	Functional Prototypes: By the end of this phase, the project should have functional prototypes of data adaptors and an API server that are effectively integrated with GPT models, capable of performing basic tasks in financial predictive analytics.
•	Initial Test Results: Initial testing should yield valuable insights into the system's performance, including data management efficiency, model interaction capabilities, and areas for improvement in predictive analytics accuracy.
•	Iterative Improvement Plan: Based on test feedback, an iterative improvement plan should be in place, outlining steps for enhancing the system's design, functionality, and integration in preparation for more extensive testing and refinement in subsequent phases.

Summary
Phase 2 stands as a crucial bridge between the theoretical foundations laid in Phase 1 and the advanced development and comprehensive testing that follow. It is characterised by a focus on development, integration, and the beginning of an iterative testing and refinement cycle. The successful completion of this phase establishes a solid foundation for the project, demonstrating the feasibility of integrating off-the-shelf AI models with financial analytics tasks and setting the stage for more complex developments and rigorous testing in the phases to come. This phase underscores the project’s transition from concept to practical application, marking a significant milestone in the research's progression towards its ultimate objectives.

Phase 3: Advanced Development and Comprehensive Testing (Weeks 13-21)

Overview
Phase 3 is characterised by a significant shift towards the technical maturation of the project, focusing on the enhancement of the training and fine-tuning environment for the GPT models, rigorous model training, iterative fine-tuning, and the execution of comprehensive system testing. This phase aims to solidify the system's capabilities, ensuring its robustness, accuracy, and readiness for real-world financial predictive analytics applications.

Goals
1.	Optimise the Training and Fine-Tuning Environment: Refine the computational infrastructure to support extensive model training and fine-tuning, ensuring optimal performance and efficiency.
2.	Intensive Model Training and Iterative Fine-Tuning: Conduct in-depth training sessions with the GPT models on diverse financial datasets to enhance their predictive accuracy and adaptability.
3.	Execute Comprehensive System Testing: Implement a thorough testing regime that not only validates the system's functional and analytical capabilities but also its resilience and scalability under various conditions.
4.	Prepare for Real-World Deployment: Ensure the system's readiness for deployment in real-world scenarios, focusing on predictive accuracy, user interaction, and scalability.

Tasks and Activities
•	Enhancement of the Computational Environment: Evaluate and upgrade the computational resources and tools required for effective model training and fine-tuning, including GPUs, data storage solutions, and relevant software libraries.
•	Data Preprocessing and Augmentation: Implement advanced data preprocessing techniques to cleanse, structure, and augment financial datasets, ensuring the models are trained on high-quality, representative data.
•	Intensive Training Regimen: Design and execute a comprehensive training schedule for the GPT models, utilising a variety of financial datasets to cover a broad spectrum of predictive analytics scenarios.
•	Iterative Model Fine-Tuning: Utilise performance feedback and analytical insights to fine-tune the models iteratively, focusing on improving predictive accuracy and reducing bias.
•	Comprehensive Testing Strategy: Develop and carry out a detailed testing plan that encompasses unit testing, integration testing, stress testing, and scenario-based testing to assess every aspect of the system's performance and reliability.
•	Documentation and Analysis: Maintain detailed documentation of the training, fine-tuning, and testing processes, including methodologies used, challenges encountered, and insights gained. This documentation will be crucial for understanding the system's capabilities and areas for further improvement.

Expected Outcomes
•	Optimised Training Environment: A highly efficient and scalable training environment, tailored to the demands of intensive GPT model training and fine-tuning.
•	Enhanced Model Performance: GPT models that exhibit superior predictive accuracy, reliability, and a deeper understanding of complex financial datasets and scenarios.
•	Validated System Functionality: Comprehensive testing results that confirm the system's robustness, accuracy, and efficiency, demonstrating its readiness for deployment in real-world financial analytics applications.
•	Blueprint for Real-World Application: A clear and actionable plan for deploying the system in real-world environments, including considerations for user interaction, scalability, and ongoing model improvement.

Summary
Phase 3 represents a critical juncture in the research project, marking the transition from developmental experimentation to the realisation of a robust, efficient, and accurate AI-driven financial predictive analytics system. This phase is dedicated to ensuring that the developed system not only meets theoretical expectations but is also capable of standing up to the rigours and demands of real-world application. Through a meticulous focus on optimisation, testing, and validation, Phase 3 sets the stage for the final adjustments, documentation, and preparation for dissertation completion in Phase 4, culminating in a project that pushes the boundaries of current AI applications in financial analytics.


Phase 4: Final Adjustments, Documentation, and Dissertation Preparation (Weeks 22-28)

Overview
This concluding phase is dedicated to refining the project based on comprehensive testing feedback, ensuring that all aspects of the system are fully operational, optimised, and ready for real-world application. It involves a meticulous review and finalisation of all project documentation, the preparation and refinement of the dissertation, and strategising for post-submission engagement with the broader academic and industry communities.

Goals
1.	System Optimisation: Implement final adjustments to the system, ensuring maximum efficiency, accuracy, and reliability based on feedback from comprehensive testing.
2.	Comprehensive Documentation: Finalise all project documentation, including detailed descriptions of system architecture, development processes, testing methodologies, and outcomes.
3.	Dissertation Finalisation: Prepare, revise, and finalise the dissertation, encapsulating the research journey, findings, and contributions to knowledge.
4.	Post-Submission Strategy: Develop a strategy for engaging with the academic and professional communities to share insights and findings, exploring opportunities for future research and application.

Tasks and Activities
•	Analysis and Refinement: Carefully analyse testing results and feedback to identify any remaining issues or potential improvements, making necessary refinements to the system to ensure its readiness for deployment and further research.
•	Documentation Overhaul: Review and update all project documentation to reflect the final system configuration and functionalities, ensuring clarity, completeness, and accessibility for future researchers and practitioners.
•	Dissertation Drafting and Revisions: Compile and synthesise the project's work into a comprehensive dissertation, incorporating feedback from advisors and peers to enhance the clarity, depth, and impact of the presented research.
•	Pre-Defence Preparations: Prepare for the dissertation defence, organising materials, and anticipating questions to effectively communicate the significance and contributions of the research.
•	Engagement Planning: Outline plans for disseminating research findings through publications, presentations, and collaborations, considering how the project's insights can inform future work and applications within and beyond the academic community.

Expected Outcomes
•	Optimised and Validated System: A fully operational system, rigorously tested and refined, demonstrating significant contributions to financial predictive analytics through the use of Transformer AI models.
•	Comprehensive Project Archive: A complete and accessible documentation set, providing a blueprint for replicating or extending the research and its applications.
•	Ready-to-Submit Dissertation: A polished dissertation ready for submission, clearly articulating the research questions, methodology, findings, and implications, contributing valuable insights to the field of AI and financial analytics.
•	Future Research and Engagement Plan: A strategic plan for sharing research outcomes with the academic and professional communities, identifying opportunities for future research, publications, and collaborations.

Summary
Phase 4 brings the research project to its zenith, encapsulating the theoretical exploration, technological innovation, and empirical findings in a comprehensive dissertation. This phase is crucial for ensuring that the project not only concludes with a high degree of academic rigour and technological sophistication but also sets the groundwork for ongoing contributions to the field. By meticulously refining the system, documenting the research process, and preparing a clear, impactful dissertation, this phase solidifies the project's place within the broader discourse of AI applications in finance, marking a significant step forward in both academic knowledge and practical application.

}}}

->>> This is what I have written so far:
{{{
\section{Introduction}
\subsection{Background and Context}
Artificial intelligence (AI) advancements have significantly impacted various industries, including finance. Among these developments, Transformer models, particularly the Generative Pre-trained Transformer (GPT) series, have demonstrated substantial capabilities in natural language processing (NLP). These models excel at generating text, understanding the context, and performing various language-related tasks with high accuracy.

In finance, predictive analytics are essential for making informed decisions, managing risks, and optimizing investment strategies. Traditional financial predictive models, such as those based on statistical methods or simpler machine learning algorithms, often fail to capture the complex patterns and nonlinear relationships inherent in financial data. This gap presents an opportunity to explore whether advanced AI models like GPT can be adapted and fine-tuned for financial predictive analytics.

Traditional financial predictive models commonly use techniques such as linear regression, logistic regression, decision trees, random forests, and time-series models such as ARIMA. Linear regression models, while simple and easy to implement, assume a linear relationship between input and output, which is often not the case in financial markets. Logistic regression is used for binary classification tasks, but it also assumes linearity in the decision boundary. Decision trees and random forests can capture non-linear relationships, but may suffer from overfitting and interpretability issues \parencite{islam2020comparison, lu2020review}.

Machine learning models in finance, such as support vector machines and neural networks, offer more flexibility in capturing complex patterns, but come with their own challenges. Neural networks, particularly deep learning models, require large amounts of data and computational resources. They also tend to be "black boxes," making their predictions difficult to interpret, which can be problematic in regulated industries like finance \parencite{mckinsey2020machine}. Time series models, such as ARIMA, are commonly used but have limitations in capturing seasonality and non-stationarity in data \parencite{tang2022survey}.

Given these limitations, there is a significant cost associated with developing bespoke AI solutions for niche applications such as financial predictive analytics. Training an AI model from scratch requires large amounts of data, computational power, and expertise, making it an expensive endeavour. This research aims to explore whether pre-trained models like GPT, which have been trained on extensive datasets for general language understanding, can be fine-tuned for specific tasks in finance. The reuse of pre-trained models, coupled with strategic fine-tuning, promises a cost-effective alternative that retains high performance.

Although the financial market is used as a testbed in this research, the findings could potentially demonstrate the adaptability of GPT models to other industries, such as law, accounting, and more. The results of this research could provide valuable insight into the broader applicability of these models, offering a cost-effective solution for various specialised applications. This dissertation examines the practicality and potential of repurposing pre-trained Transformer models for specialised applications beyond their original training scope, with a specific focus on financial predictive analytics.


\subsection{Problem Statement}
The financial sector is highly dependent on predictive analytics to inform decision-making processes, manage risks, and optimise investment strategies. Traditional predictive models, including linear regression, logistic regression, decision trees, and random forests, often fall short in capturing the complex, non-linear patterns inherent in financial data. These models either oversimplify the relationships or struggle with overfitting and interpretability issues, which limits their effectiveness and reliability in a highly dynamic market environment.

Machine learning approaches, such as support vector machines and neural networks, offer increased flexibility in identifying intricate data patterns. However, these methods require extensive data and significant computational resources and can act as "black boxes," making their predictions difficult to interpret. This opacity poses a considerable challenge in the finance industry, where regulatory compliance and the need for transparent decision-making are critical \parencite{mckinsey2020machine, tang2022survey}.

The advent of Transformer models, particularly the Generative Pre-trained Transformer (GPT), has showcased remarkable capabilities in natural language processing tasks. These models excel in understanding context, generating coherent text, and performing various language-related tasks with high accuracy. Despite their potential, there is limited research on their application in financial predictive analytics \parencite{fourie2023changing}.

Developing bespoke AI solutions for financial predictive analytics is resource-intensive, requiring vast datasets, computational power, and specialised expertise. This research aims to address whether pre-trained models like GPT can be effectively fine-tuned for financial predictive tasks, providing a cost-effective and high-performing alternative to developing models from scratch. The study investigates the feasibility of adapting GPT models to capture the intricate patterns and relationships in financial data, thus enhancing the accuracy and reliability of financial predictions.

This dissertation explores the adaptation of GPT models for financial predictive analytics, evaluating their performance, cost-effectiveness, and potential applicability to other specialised domains. By addressing these challenges, the research seeks to contribute to the development of advanced, interpretable, and efficient AI models tailored to the specific needs of the financial sector.

\subsection{Research Objectives}
The primary aim of this research is to evaluate the feasibility and effectiveness of fine-tuning pre-trained Transformer AI models, specifically GPT, for financial predictive analytics. This overarching goal is supported by several specific objectives:

\begin{itemize}
    \item \textbf{Assess the Adaptability of Pre-trained GPT Models}
    \begin{itemize}
        \item Investigate whether GPT models, initially trained for general language processing tasks, can be effectively repurposed for financial predictive analytics \parencite{pavlyshenko2023}.
        \item Identify the extent to which these models can be fine-tuned to handle the unique challenges and complexities of financial data, including non-linear relationships and market volatility \parencite{li2023}.
        \item Examine the performance of these models in various financial predictive tasks to determine their adaptability and versatility \parencite{zhang2023}.
    \end{itemize}
    
    \item \textbf{Develop a Dynamic Orchestration System}
    \begin{itemize}
        \item Design and implement a Node.js-based orchestration system capable of managing multiple GPT model instances.
        \item Ensure the system can dynamically integrate and coordinate these models to perform various financial predictive tasks, such as sentiment analysis, news impact correlation, and trend forecasting \parencite{wang2023}.
        \item Develop bespoke adaptors to facilitate seamless communication between the orchestration system and the GPT models.
    \end{itemize}
    
    \item \textbf{Evaluate the System's Performance and Cost-effectiveness}
    \begin{itemize}
        \item Conduct rigorous testing to assess the predictive accuracy, efficiency, and scalability of the fine-tuned GPT models within the developed system \parencite{pavlyshenko2023}.
        \item Compare the performance and cost-effectiveness of the pre-trained models against traditional financial predictive models and bespoke AI solutions \parencite{li2023}.
        \item Analyse the results to identify potential areas for improvement and optimisation.
    \end{itemize}
    
    \item \textbf{Enhance Financial Predictive Analytics Through Fine-tuning}
    \begin{itemize}
        \item Implement iterative fine-tuning processes to continuously improve the models' accuracy and reliability in predicting stock market trends \parencite{zhang2023}.
        \item Explore advanced data preprocessing and augmentation techniques to ensure high-quality input data for model training \parencite{pavlyshenko2023}.
        \item Develop strategies to refine the models' ability to capture and analyse complex financial patterns \parencite{li2023}.
    \end{itemize}
    
    \item \textbf{Demonstrate Practical Applicability and Scalability}
    \begin{itemize}
        \item Validate the system's readiness for real-world deployment by conducting comprehensive system testing and performance evaluations \parencite{wang2023}.
        \item Develop a scalable framework that can be adapted for various financial predictive tasks and potentially extended to other specialised domains beyond finance \parencite{zhang2023}.
        \item Ensure the system is robust and reliable, capable of handling real-world financial data and market conditions \parencite{li2023}.
    \end{itemize}
    
    \item \textbf{Contribute to the Broader Field of AI and Financial Analytics}
    \begin{itemize}
        \item Provide insights into the broader applicability of pre-trained Transformer models across different industries, highlighting their potential to deliver sophisticated predictive analytics in cost-effective ways \parencite{pavlyshenko2023}.
        \item Publish findings and methodologies to contribute to the academic and professional discourse on AI model repurposing and fine-tuning \parencite{li2023}.
        \item Share the results with the wider research community to encourage further exploration and innovation in the field \parencite{zhang2023}.
    \end{itemize}
\end{itemize}

By achieving these objectives, this research aims to demonstrate the potential of leveraging pre-trained GPT models for financial predictive analytics, offering a viable alternative to traditional and bespoke AI development approaches. This study seeks to contribute to the development of advanced, interpretable, and efficient AI models tailored to the specific needs of the financial sector.


\subsection{Significance of the Study}
The significance of this study lies in its exploration of the feasibility of applying pre-trained Transformer AI models, specifically GPT, to the domain of financial predictive analytics. Traditional models often face difficulties in capturing the complex and nonlinear nature of financial data. This research aims to investigate whether these pre-trained models can be fine-tuned effectively for financial tasks \parencite{ballout2023, otiefy2024}.

This study seeks to determine if leveraging pre-trained GPT models can potentially offer a cost-effective alternative to bespoke AI solutions, which typically require substantial resources. By examining this possibility, the research aims to provide insights into whether advanced predictive analytics can become more accessible to organisations that may not have the resources to develop custom AI models from scratch. The primary focus is on assessing the feasibility of this approach, rather than providing a definitive replacement or solution \parencite{krstić2023}.

A significant aspect of this study is the development of a dynamic orchestration system that manages multiple GPT model instances. This system is designed to facilitate the integration and coordination of various models to perform distinct predictive tasks, potentially enhancing the accuracy and reliability of financial predictions. The methodologies developed could have broader implications for other industries where predictive analytics are essential, highlighting the versatility of pre-trained models when applied to specialised tasks \parencite{li2023}.

Furthermore, this research contributes to the broader field of AI by examining the adaptability of pre-trained models across different domains. The study's methodologies and findings can serve as a foundation for future research, encouraging further exploration into the use of Transformer models for specialised applications beyond their initial training objectives. By investigating these models' ability to handle new and unanticipated tasks, the research sheds light on the flexibility and limitations of pre-trained AI in adapting to off-label uses \parencite{ballout2023, krstić2023}.

The potential benefits of this research extend beyond immediate financial applications. If pre-trained models like GPT can be effectively fine-tuned for financial predictive analytics, this approach could lead to more widespread adoption of AI technologies in various sectors. For example, industries such as healthcare, legal, and accounting could similarly benefit from applying these models to domain-specific predictive tasks. This broader applicability underscores the importance of evaluating the feasibility and performance of pre-trained models in diverse contexts \parencite{li2023, otiefy2024}.

Moreover, this study addresses the critical need for cost-effective AI solutions. By exploring whether pre-trained models can be repurposed efficiently, the research provides valuable insights into the potential for reducing the high costs associated with developing bespoke AI systems. This aspect is particularly relevant for smaller organisations and startups that may lack the financial resources to invest in custom AI development. The findings could help democratise access to advanced AI technologies, fostering innovation and competition across various industries \parencite{krstić2023}.

The research also highlights the importance of iterative fine-tuning processes. By continuously refining the models based on performance evaluations and cumulative analyses, the study aims to improve the predictive accuracy and reliability of the models over time. This iterative approach is crucial for adapting the models to the dynamic nature of financial markets and ensuring their long-term efficacy \parencite{ballout2023, otiefy2024}.

In evaluating the feasibility of using pre-trained Transformer models for financial predictive analytics, this study offers a comprehensive examination of their strengths and limitations. Even if the results indicate that these models are not yet ready for intense off-label applications, the research still achieves its aims by providing a clear understanding of the current capabilities and areas for improvement. This knowledge can guide future efforts to enhance the adaptability and performance of pre-trained AI models in specialised domains \parencite{li2023}.

Ultimately, the significance of this study lies in its potential to inform and guide future research and development in AI. By rigorously evaluating the feasibility of fine-tuning pre-trained Transformer models for financial tasks, the research contributes valuable insights to the ongoing discourse on AI applications, offering a pragmatic assessment of the possibilities and challenges associated with repurposing these powerful tools for new and diverse uses \parencite{ballout2023, krstić2023, otiefy2024}.


\subsection{Structure of the Dissertation}
This dissertation is structured to systematically explore the feasibility of applying pre-trained Transformer AI models, specifically GPT, to the domain of financial predictive analytics. It follows a logical progression from the foundational context and objectives to the detailed methodology, implementation, and evaluation of the research. The structure is as follows:

\subsubsection{Chapter 1: Research Plan}
This chapter outlines the overall plan for the research, providing a context for the study, describing the technology stack, and detailing the project objectives and approach. It also includes key features, the impact and potential of the research, and concludes with the research context and the preliminary literature review \parencite{coker2023}.

\subsubsection{Chapter 2: Introduction}
The introduction sets the stage for the dissertation by providing background information and context. It discusses the significance of AI advancements, particularly Transformer models, in various industries including finance. The chapter then presents the problem statement, research objectives, and the significance of the study, which highlights the potential benefits and broader applicability of the research \parencite{roberts2004}.

\subsubsection{Chapter 3: Literature Review}
This chapter provides a comprehensive review of the existing literature related to the study. It includes:
\begin{itemize}
    \item An overview of Transformer AI models.
    \item Applications of Transformer models in finance.
    \item Node.js and AI orchestration.
    \item Data management in financial analytics.
    \item Ethical considerations in AI for financial analytics, including data privacy, algorithmic bias, transparency, and societal impacts.
    \item Identification of gaps in existing research \parencite{ronau2014}.
\end{itemize}

\subsubsection{Chapter 4: Methodology}
The methodology chapter details the research design and the specific methods used to conduct the study. It covers:
\begin{itemize}
    \item The technology stack and tools, including JavaScript, Node.js, and GPT models.
    \item The development phases, from the foundation and conceptual framework to system development, advanced testing, and final adjustments.
    \item Data sources and collection methods.
    \item Evaluation metrics used to assess the performance and feasibility of the models \parencite{skobkin2014}.
\end{itemize}

\subsubsection{Chapter 5: System Design and Architecture}
This chapter discusses the conceptual framework and the overall design of the system. It includes:
\begin{itemize}
    \item The main components of the system such as data adaptors, API server, and model orchestration.
    \item The data flow and interaction between these components.
    \item The integration strategy employed to ensure seamless operation of the system.
\end{itemize}

\subsubsection{Chapter 6: Implementation}
The implementation chapter describes the practical steps taken to develop and test the system. It is divided into four phases:
\begin{itemize}
    \item Phase 1: Foundation and conceptual framework, including literature survey insights and initial prototype development.
    \item Phase 2: System development and initial testing, focusing on the development of the API server and integration of data adaptors with GPT models.
    \item Phase 3: Advanced development and comprehensive testing, detailing the enhancement of the training environment, intensive model training, and comprehensive system testing.
    \item Phase 4: Final adjustments and documentation, involving system refinements and final validation.
\end{itemize}

\subsubsection{Chapter 7: Results}
This chapter presents the results of the testing and evaluation phases. It includes:
\begin{itemize}
    \item A summary of initial testing outcomes.
    \item Insights from comprehensive system testing.
    \item Performance evaluation in terms of predictive accuracy and data management efficiency.
    \item Validation of results against the research objectives.
\end{itemize}

\subsubsection{Chapter 8: Discussion}
The discussion chapter interprets the results and places them in the context of existing literature. It covers:
\begin{itemize}
    \item Interpretation of results and comparison with existing studies.
    \item Implications for financial predictive analytics.
    \item Challenges and limitations encountered during the research.
    \item Ethical implications of using AI in financial analytics.
    \item Practical applications and technical innovations.
    \item Potential areas for future research.
\end{itemize}

\subsubsection{Chapter 9: Conclusion}
The conclusion summarizes the key findings of the research and their contributions to knowledge. It also discusses practical applications of the research and provides recommendations for future work. This chapter ties together the entire dissertation, highlighting the achievements and future potential of the study.

Each chapter is designed to build upon the previous one, ensuring a coherent and comprehensive exploration of the research topic. This structure allows for a detailed examination of the feasibility and potential of repurposing pre-trained Transformer models for financial predictive analytics, contributing valuable insights to the fields of AI and finance.


\section{Literature Review}
\subsection{Overview of Transformer AI Models}
Text

\subsection{Applications of Transformer Models in Finance}
Text

\subsection{Node.js and AI Orchestration}
Text

\subsection{Data Management in Financial Analytics}
Text

\subsection{Ethical Considerations in AI for Financial Analytics}
Text

\subsubsection{Data Privacy and Security Concerns}
Text

\subsubsection{Algorithmic Bias in AI Models}
Text

\subsubsection{Transparency and Accountability}
Text

\subsubsection{Societal Impacts}
Text

\subsection{Gaps in Existing Research}
Text

\section{Methodology}
\subsection{Research Design}
The research design of this dissertation systematically evaluates the feasibility and effectiveness of applying pre-trained Transformer AI models, specifically GPT, to financial predictive analytics. This section outlines the research strategy, data collection methods, analytical approaches, model development, system design, validation techniques, and ethical considerations employed to achieve the study's objectives.

\subsubsection{Research Strategy}
The study employs an exploratory research strategy, focusing on the practical application of GPT models in financial predictive analytics. This approach suits the innovative nature of applying pre-trained language models to financial tasks, where traditional models are predominantly used \parencite{shidaganti2023}. The exploratory strategy allows for flexible adaptation and iterative refinement of methodologies, essential for addressing the challenges and opportunities presented by this application.

The research is divided into several distinct phases, each targeting specific aspects of the feasibility and performance of the models. These phases include:
\begin{enumerate}
    \item \textbf{Literature Review}: A comprehensive review of existing literature on Transformer models, particularly GPT, and their applications in various domains. The review aims to identify gaps in the current research, particularly in the context of financial predictive analytics, and provides a theoretical foundation for the subsequent phases by summarising relevant techniques and methodologies \parencite{rofii2023}.
    \item \textbf{System Design and Development}: Based on insights from the literature review, this phase focuses on designing the system architecture and developing necessary components. This includes creating data adaptors for preprocessing financial data, setting up the API server for managing interactions between data sources and GPT models, and developing a dynamic orchestration system to handle multiple model instances.
    \item \textbf{Implementation}: This phase involves integrating all the components developed in the previous phase. The implementation process includes setting up the computational infrastructure, fine-tuning the pre-trained GPT models on financial datasets, and ensuring seamless interaction between the system components.
    \item \textbf{Testing}: This phase involves rigorous testing of the system to evaluate its performance. Various methods, including backtesting with historical data and cross-validation, assess the predictive accuracy and reliability of the fine-tuned GPT models \parencite{pfutzenreuter2022}. The testing phase also identifies any bottlenecks or inefficiencies, providing valuable feedback for iterative improvements.
    \item \textbf{Evaluation}: The final phase focuses on a comprehensive evaluation of the system’s performance, analysing results obtained from the testing phase. This includes comparing the predictive accuracy of the GPT models with traditional financial predictive models and assessing the system’s scalability, efficiency, and overall feasibility for real-world financial analytics applications \parencite{kurniawan2023}.
\end{enumerate}

\subsubsection{Data Collection Methods}
Data collection involves acquiring extensive financial datasets and relevant news articles, which serve as the primary sources for training and evaluating the GPT models. The selected datasets provide a comprehensive representation of market conditions and news events, ensuring a robust basis for model training and evaluation.

\begin{enumerate}
    \item \textbf{Stock Market Data}: Historical stock prices and trading volumes from reputable financial databases such as Yahoo Finance and Bloomberg. This data includes daily closing prices, trading volumes, and other relevant financial metrics over a significant period, serving as the foundation for training the predictive models \parencite{rofii2023}.
    \item \textbf{Financial News Articles}: News articles from established financial news platforms like Reuters, Financial Times, and Bloomberg provide context and sentiment information. These articles cover a wide range of financial events, market analyses, and economic news that can influence stock market movements. Natural language processing (NLP) techniques process these articles to extract relevant information and sentiment scores \parencite{shidaganti2023}.
    \item \textbf{Sentiment Data}: Sentiment scores are derived from financial news articles using NLP tools. These tools analyse the textual content to determine overall sentiment (positive, negative, or neutral) and assign a sentiment score. The sentiment data is crucial for understanding market reactions to news events and incorporating this information into the predictive models \parencite{pfutzenreuter2022}.
\end{enumerate}

The data collection process is meticulously designed to ensure the quality and relevance of the datasets. Financial datasets cover diverse market conditions, including bullish, bearish, and volatile periods. News articles encompass various financial topics, ensuring a diverse corpus for sentiment analysis.

\subsubsection{Analytical Approaches}
The research adopts several analytical approaches to assess the performance of GPT models in financial predictive analytics, each targeting a specific aspect of model effectiveness and utility.

\begin{enumerate}
    \item \textbf{Sentiment Analysis}:
    \begin{itemize}
        \item \textbf{Objective}: Leverage the linguistic capabilities of GPT models for extracting sentiment from financial news articles.
        \item \textbf{Methodology}: Fine-tune pre-trained GPT models using a corpus of financial news articles to generate sentiment scores \parencite{li2023}.
        \item \textbf{Implementation}: Generate sentiment scores for each article, classify the sentiment as very positive, positive, very negative, negative, or neutral, and aggregate these scores over time to create a sentiment index \parencite{pavlyshenko2023}.
        \item \textbf{Correlation Analysis}: Correlate the sentiment index with historical stock price movements to identify patterns and gauge the impact of market sentiment on stock prices. Use statistical measures like Pearson's correlation coefficient to quantify the relationship between sentiment scores and market performance \parencite{pavlyshenko2023}.
    \end{itemize}
    \item \textbf{Predictive Modelling}:
    \begin{itemize}
        \item \textbf{Objective}: Predict stock market trends using historical data and sentiment analysis.
        \item \textbf{Model Development}: Fine-tune GPT models on historical stock market data, incorporating features like closing prices, trading volumes, and the sentiment index derived from news articles \parencite{sathiyamurthi2024}.
        \item \textbf{Training}: Feed historical data into the GPT models and iteratively adjust the model parameters to minimise prediction errors.
        \item \textbf{Validation}: Validate predictive accuracy using a holdout dataset and cross-validation techniques to ensure robustness and prevent overfitting. Use metrics like mean squared error (MSE), mean absolute error (MAE), and directional accuracy to evaluate model performance \parencite{rofii2023}.
    \end{itemize}
    \item \textbf{Performance Evaluation}:
    \begin{itemize}
        \item \textbf{Objective}: Benchmark the predictive performance of GPT models against traditional financial models.
        \item \textbf{Comparative Analysis}: Compare predictions from GPT models with established models like AutoRegressive Integrated Moving Average (ARIMA), linear regression, and neural networks \parencite{li2023}.
        \item \textbf{Evaluation Metrics}: Key performance indicators include prediction accuracy, precision, recall, F1 score, and computational efficiency. Evaluate the models for their ability to capture non-linear relationships and complex patterns in the data \parencite{sathiyamurthi2024}.
    \end{itemize}
\end{enumerate}

\subsubsection{Model Development and Fine-Tuning}
The core of this research involves developing and fine-tuning GPT models specifically tailored for financial predictive analytics. This process encompasses several critical steps to ensure the models are effectively adapted to the unique demands of financial data.

\begin{enumerate}
    \item \textbf{Model Selection}:
    \begin{itemize}
        \item \textbf{Criteria}: Select pre-trained GPT models based on architecture, size (number of parameters), training data, and demonstrated performance in various NLP tasks \parencite{li2023}.
        \item \textbf{Suitability}: Choose models for their potential to be fine-tuned for specific financial tasks. Prefer larger models with more parameters for their capacity to capture complex patterns, though computational efficiency is also considered.
        \item \textbf{Pre-training}: The chosen models have been pre-trained on diverse and extensive datasets, providing a robust foundation for further fine-tuning on financial data \parencite{pavlyshenko2023}.
    \end{itemize}
    \item \textbf{Fine-Tuning Process}:
    \begin{itemize}
        \item \textbf{Data Preparation}: Preprocess financial datasets, including historical stock prices and sentiment data, to ensure compatibility with the GPT models. This involves normalising data, handling missing values, and structuring input sequences.
        \item \textbf{Iterative Training}: Conduct multiple rounds of training where the models are exposed to the financial datasets. Adjust model weights during each iteration to improve predictive accuracy.
        \item \textbf{Techniques}: Employ techniques like transfer learning and domain adaptation to enhance the models’ understanding of financial contexts. Include regular validation during fine-tuning to monitor performance and adjust training parameters as needed \parencite{sathiyamurthi2024}.
    \end{itemize}
    \item \textbf{Hyperparameter Tuning}:
    \begin{itemize}
        \item \textbf{Objective}: Optimise the model's performance by fine-tuning hyperparameters that govern the training process.
        \item \textbf{Parameters}: Key hyperparameters include learning rate, batch size, number of epochs, dropout rate, and the optimiser used (e.g., Adam, SGD).
        \item \textbf{Optimisation Methods}: Apply grid search and random search methods to systematically explore the hyperparameter space. Use Bayesian optimisation for more efficient tuning \parencite{rofii2023}.
        \item \textbf{Evaluation}: Evaluate the impact of different hyperparameter settings based on the

 model’s performance metrics. Select the optimal set of hyperparameters to maximise predictive accuracy and minimise errors \parencite{sathiyamurthi2024}.
    \end{itemize}
\end{enumerate}

By developing and fine-tuning the GPT models, the research aims to evaluate their viability for financial predictive analytics, providing an examination of their capabilities and performance.

\subsubsection{System Design and Architecture}
The system design integrates various essential components for the efficient training, deployment, and operation of GPT models in financial predictive analytics. The architecture ensures seamless interaction between data sources, models, and end-users, facilitating a robust and scalable analytical framework.

\begin{enumerate}
    \item \textbf{Data Adaptor Modules}:
    \begin{itemize}
        \item \textbf{Purpose}: Handle the pre-processing and structuring of raw financial data, transforming them into a format suitable for model training and analysis.
        \item \textbf{Functionality}: Perform operations including data cleaning, normalisation, feature extraction, and enrichment. Ensure the data fed into the GPT models is accurate, consistent, and comprehensive \parencite{nararya2021}.
        \item \textbf{Components}: Include parsers for various data formats (e.g., CSV, JSON), normalisation algorithms, and feature engineering tools to create relevant predictive variables.
        \item \textbf{Integration}: Integrate with data sources such as Yahoo Finance, Bloomberg, and news APIs, enabling real-time data acquisition and processing. This integration is crucial for maintaining up-to-date datasets for continuous model training and validation \parencite{elhabbash2023}.
    \end{itemize}
    \item \textbf{API Server}:
    \begin{itemize}
        \item \textbf{Purpose}: Acts as the central hub for managing communications between different system components, including data sources, GPT models, and user interfaces.
        \item \textbf{Functionality}: Handles API requests for data retrieval, model training, prediction generation, and user interactions. Ensures secure and efficient data flow across the system \parencite{hinojosa2021}.
        \item \textbf{Scalability}: Supports asynchronous operations and load balancing, enabling it to handle multiple concurrent requests and large volumes of data.
        \item \textbf{Endpoints}: Provides various endpoints for data ingestion, model interaction, and result dissemination. These endpoints facilitate the integration of external tools and user applications, allowing for a flexible and extensible system.
    \end{itemize}
    \item \textbf{Model Orchestration}:
    \begin{itemize}
        \item \textbf{Purpose}: Dynamically manages multiple GPT model instances to optimise resource utilisation and task distribution.
        \item \textbf{Functionality}: Allocates computational resources based on task requirements, ensuring that each model instance operates efficiently. Coordinates the execution of different predictive tasks, such as sentiment analysis, trend forecasting, and news impact correlation.
        \item \textbf{Dynamic Management}: Supports the dynamic deployment of models, enabling real-time scaling and adjustment based on workload and performance metrics.
        \item \textbf{Resource Utilisation}: Monitors resource usage and model performance, redistributing tasks and reallocating resources to maintain optimal operation. Ensures the system can handle varying loads and maintain high performance without unnecessary resource consumption \parencite{sinha2024}.
    \end{itemize}
\end{enumerate}

\subsubsection{Validation Techniques}
To ensure the reliability and accuracy of the proposed system, the research employs a comprehensive set of validation techniques. These techniques rigorously evaluate the performance of the predictive models and the overall system.

\begin{enumerate}
    \item \textbf{Backtesting}:
    \begin{itemize}
        \item \textbf{Objective}: Evaluate the predictive models using historical financial data to simulate their performance in past market conditions.
        \item \textbf{Implementation}: Use historical stock prices and trading volumes to generate predictions for past periods. Compare the predicted values with actual market outcomes to assess the models' accuracy.
        \item \textbf{Analysis}: Calculate key performance indicators, such as prediction error rates and directional accuracy, to measure the models' effectiveness. Identify potential overfitting and other performance issues in a controlled environment \parencite{nararya2021}.
    \end{itemize}
    \item \textbf{Cross-Validation}:
    \begin{itemize}
        \item \textbf{Objective}: Ensure the robustness and generalisability of the predictive models across different subsets of data.
        \item \textbf{Methodology}: Partition the dataset into multiple folds, and train and validate the models iteratively on different combinations of these folds. This process assesses how well the models perform on unseen data.
        \item \textbf{Types}: Employ techniques such as k-fold cross-validation and time series split to account for the temporal nature of financial data. These methods provide a thorough evaluation of model stability and reliability \parencite{elhabbash2023}.
    \end{itemize}
    \item \textbf{Performance Metrics}:
    \begin{itemize}
        \item \textbf{Objective}: Quantify the accuracy, reliability, and efficiency of the predictive models using standard performance metrics.
        \item \textbf{Metrics}:
        \begin{itemize}
            \item \textbf{Mean Absolute Error (MAE)}: Measures the average absolute difference between predicted and actual values, providing a straightforward indicator of prediction accuracy.
            \item \textbf{Root Mean Squared Error (RMSE)}: Assesses the square root of the average squared differences between predicted and actual values, giving more weight to larger errors.
            \item \textbf{R-squared (R²)}: Indicates the proportion of variance in the dependent variable predictable from the independent variables, reflecting the overall fit of the model.
        \end{itemize}
        \item \textbf{Evaluation}: Calculate these metrics for each model and compare them to benchmarks established by traditional financial models. This comparison highlights the improvements or limitations of using GPT models for financial predictive tasks \parencite{hinojosa2021}.
    \end{itemize}
\end{enumerate}

By employing these validation techniques, the research ensures a rigorous assessment of the system's performance, providing confidence in the applicability and reliability of GPT models in financial predictive analytics.

\subsubsection{Ethical Considerations}
Ethical considerations are central to this research's design and implementation. Ensuring responsible and fair use of AI in financial analysis is crucial to maintaining public trust and adherence to legal and ethical standards.

\begin{enumerate}
    \item \textbf{Data Privacy}:
    \begin{itemize}
        \item \textbf{Compliance with Regulations}: Handle all data in strict accordance with relevant data protection regulations, such as the General Data Protection Regulation (GDPR) and other applicable laws. Ensure data collection, storage, and processing meet high standards of privacy and security \parencite{wang2024}.
        \item \textbf{Anonymisation}: Anonymise data where necessary to protect the identities of individuals and entities. This involves removing personally identifiable information (PII) and implementing techniques such as data masking and pseudonymisation \parencite{balbaa2023}.
        \item \textbf{Consent and Transparency}: Source data from publicly available datasets and reputable financial databases, where data usage terms are clearly defined. Obtain explicit permission for any data involving individual consent, and transparently communicate the purposes of data usage to the data providers \parencite{ajiga2024}.
    \end{itemize}
    \item \textbf{Bias Mitigation}:
    \begin{itemize}
        \item \textbf{Identifying Bias}: Employ techniques to detect potential biases in the training data and model outputs. Analyse data for skewness, imbalances, and representation issues that could lead to biased predictions \parencite{farayola2024}.
        \item \textbf{Mitigating Bias}: Implement strategies like re-sampling, re-weighting, and augmenting the training data to ensure balanced representation of different market conditions and scenarios. Incorporate fairness constraints into the model training process to reduce bias \parencite{kalia2023}.
        \item \textbf{Continuous Monitoring}: Monitor bias detection and mitigation continuously throughout the model development lifecycle. Conduct regular audits and performance evaluations to ensure that the models remain fair and unbiased over time \parencite{balbaa2023}.
    \end{itemize}
    \item \textbf{Transparency and Accountability}:
    \begin{itemize}
        \item \textbf{Model Explainability}: Enhance the explainability of the GPT models to ensure their predictions can be understood and interpreted by users. Employ techniques such as attention mechanism analysis and feature importance scoring to provide insights into the models' decision-making processes \parencite{ajiga2024}.
        \item \textbf{Accountability Mechanisms}: Establish clear accountability mechanisms to address any ethical concerns that arise during the research. Document decision-making processes, maintain transparency in model development, and provide channels for reporting and addressing ethical issues \parencite{wang2024}.
    \end{itemize}
\end{enumerate}

By incorporating these ethical considerations, the research promotes responsible AI use in financial analytics, ensuring that the benefits of AI are realised without compromising ethical standards and public trust.

\subsubsection{Conclusion of Research Design}
The research design provides a structured and methodical approach to exploring the feasibility and effectiveness of applying GPT models to financial predictive analytics. The design integrates robust data collection methods, comprehensive analytical approaches, and rigorous validation techniques to ensure a thorough evaluation of the model performance.

By leveraging extensive financial datasets, advanced sentiment analysis, and predictive

 modelling, the research aims to uncover valuable insights into the potential of pre-trained GPT models in the financial domain. The incorporation of ethical considerations ensures that the research is conducted responsibly, focusing on data privacy, bias mitigation, and transparency \parencite{farayola2024}.

The phased approach to system development, from initial design to comprehensive testing and final validation, ensures that each aspect of the research is meticulously planned and executed. This structured methodology improves the reliability of the findings and provides a clear roadmap for future research and application in AI and finance \parencite{kalia2023}.

This research aims to contribute to the understanding and application of artificial intelligence in financial predictive analytics, potentially leading to more accurate, efficient, and ethical financial forecasting models. The insights gained from this study will help bridge the gap between theoretical AI advancements and practical financial applications, paving the way for innovative solutions in the financial sector \parencite{ajiga2024}.
}}}


->>> This is the structure of chapter and sections and subsections of my research yet to be written:
{{{
	4.2 Technology Stack and Tools
		4.2.1 Programming Language: JavaScript
		4.2.2 Runtime Environment: Node.js
		4.2.3 AI Models: GPT Transformer AI
	4.3 Development Phases
		4.3.1 Phase 1: Foundation and Conceptual Framework
		4.3.2 Phase 2: System Development and Initial Testing
		4.3.3 Phase 3: Advanced Development and Comprehensive Testing
		4.3.4 Phase 4: Final Adjustments, Documentation, and Dissertation Preparation
	4.4 Data Sources and Collection Methods
	4.5 Evaluation Metrics.
5 System Design and Architecture
	5.1 Conceptual Framework
	5.2 System Components
		5.2.1 Data Adaptors.
		5.2.2 API Server.
		5.2.3 Model Orchestration.
	5.3 Data Flow and Interaction.
	5.4 Integration Strategy
6 Implementation
	6.1 Phase 1: Foundation and Conceptual Framework.
		6.1.1 Literature Survey Insights
		6.1.2 Prototype Development.
		6.1.3 Initial Testing
	6.2 Phase 2: System Development and Initial Testing.
		6.2.1 API Server Development
		6.2.2 Integration of Data Adaptors and GPT Models
		6.2.3 Preliminary System Testing.
	6.3 Phase 3: Advanced Development and Comprehensive Testing
		6.3.1 Enhancement of Training Environment.
		6.3.2 Intensive Model Training and Fine-Tuning.
		6.3.3 Comprehensive System Testing
	6.4 Phase 4: Final Adjustments and Documentation
		6.4.1 System Refinements
		6.4.2 Final System Validation.
7 Results
	7.1 Summary of Initial Testing Outcomes
	7.2 Insights from Comprehensive System Testing
	7.3 Performance Evaluation
		7.3.1 Predictive Accuracy
		7.3.2 Data Management Efficiency
	7.4 Validation against Research Objectives
8 Discussion
	8.1 Interpretation of Results
	8.2 Comparison with Existing Literature
	8.3 Implications for Financial Predictive Analytics
	8.4 Challenges and Limitations
	8.5 Ethical Implications
	8.6 Practical Applications.
	8.7 Technical Innovations.
	8.8 Potential for Future Research.
9 Conclusion
	9.1 Summary of Key Findings.
	9.2 Contributions to Knowledge
	9.3 Practical Applications.
9.4 Recommendations for Future Work.
}}}


->>>Your task is to write these section below:
{{{
	4.5 Evaluation Metrics.
}}}
Strictly adhere to the instructions below:

- Use British English, British Spelling, British preferred choice of words, British expression, British preferred syntax and semantics.
- Don't use literature words like "paramount", "pivotal" or anything clever.
- Be direct and objective. Avoid adverbs and adjectives entirely if possible.
- Make sure you are being academic and scientific without making use of clever words. Be direct and to the point, the text must be informative, not pretty or clever.
- Avoid this sentence construction: "not only [this], but also [that]", instead, be direct and objective, use this sentence construction: "[this] and [that]", can you see the difference?
- Don't be arrogant, on the contrary, be humble and stick to the research, avoid to the extreme making this research sounding like a grand breakthrough or something deserving of a Nobel Prize. Be HUMBLE, stick to what it is scoped, don't overdo it.
Go.
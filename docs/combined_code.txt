
========================================
File: ./scripts/news_sort_json.py
========================================

# File: scripts/news_sort_json.py

import json

def sort_json(input_file, output_file):
    with open(input_file, 'r') as f:
        data = json.load(f)
    
    sorted_keys = sorted(data.keys(), key=lambda x: (int(x.split('_')[0]), x.split('_')[1]))
    sorted_data = {key: data[key] for key in sorted_keys}
    
    with open(output_file, 'w') as f:
        json.dump(sorted_data, f, indent=4)

input_file = './News_Data/merged_news_data.json'
output_file = './News_Data/sorted_output.json'
sort_json(input_file, output_file)

========================================
File: ./scripts/combine_code_script.py
========================================

import os

# Define the directories and file extensions to include
directories_to_include = [
    "./scripts",
    "./src/adaptors",
    "./src/news_fetch",
    "./src/orchestration",
    "./src/planner",
    "./src/stock_fetch"
]
extensions_to_include = ['.py', '.js']

# Define the output file path
output_file_path = './docs/combined_code.txt'

# Start writing to the output file
with open(output_file_path, 'w') as output_file:
    for directory in directories_to_include:
        for root, _, files in os.walk(directory):
            for file in files:
                if any(file.endswith(ext) for ext in extensions_to_include):
                    file_path = os.path.join(root, file)
                    output_file.write(f"\n{'='*40}\n")
                    output_file.write(f"File: {file_path}\n")
                    output_file.write(f"{'='*40}\n\n")
                    with open(file_path, 'r') as f:
                        output_file.write(f.read())
                        output_file.write("\n")

    # Include this script's code at the end of the file
    script_path = './scripts/combine_code_script.py'
    output_file.write(f"\n{'='*40}\n")
    output_file.write(f"File: {script_path}\n")
    output_file.write(f"{'='*40}\n\n")
    with open(__file__, 'r') as f:
        output_file.write(f.read())
        output_file.write("\n")

print(f"All code has been combined into {output_file_path}")

========================================
File: ./scripts/news_merge_json_files.py
========================================

#File: scripts/news_merge_json_files.py

import os
import json

# Define the directory containing the JSON files
directory = './News_Data'

# Initialize an empty dictionary to hold the merged data
merged_data = {}

# Loop over all files in the directory
for filename in os.listdir(directory):
    if filename.endswith('.json'):
        # Construct the full file path
        filepath = os.path.join(directory, filename)
        
        # Read the content of the JSON file
        with open(filepath, 'r', encoding='utf-8') as file:
            data = json.load(file)
        
        # Use the filename (without the extension) as the key in the merged dictionary
        key = os.path.splitext(filename)[0]
        merged_data[key] = data

# Define the output file path
output_file = os.path.join(directory, 'merged_news_data.json')

# Write the merged data to a new JSON file
with open(output_file, 'w', encoding='utf-8') as outfile:
    json.dump(merged_data, outfile, ensure_ascii=False, indent=4)

print(f"Merged data has been written to {output_file}")

========================================
File: ./scripts/stock_organise_series.py
========================================

# file: scripts/stock_organise_series.py

import json

DATA_TIME_SERIES = "Monthly Time Series"
PATH_SOURCE = "./stock_data/monthly_SPY.json"
PATH_OUTPUT = "./stock_data/reverse_monthly_SPY.json"

# Load the JSON data from a file
with open(PATH_SOURCE, 'r') as file:
    data = json.load(file)

# Extract the data
time_series = data[DATA_TIME_SERIES]

# Reverse the order of the time series
reversed_time_series = dict(reversed(list(time_series.items())))

# Add a counter to each date
counter = 1
new_time_series = {}
for date in reversed_time_series:
    new_key = f"{counter}_{date}"
    new_time_series[new_key] = reversed_time_series[date]
    counter += 1

# Update the original data with the new time series
data[DATA_TIME_SERIES] = new_time_series

# Save the updated data back to a file
with open(PATH_OUTPUT, 'w') as file:
    json.dump(data, file, indent=4)

# print("The order has been reversed and the counter has been added.")

========================================
File: ./src/adaptors/gptc-adaptor.js
========================================

// File: src/adaptors/gptc-adaptor.js

const { OpenAI } = require("openai");
const fs = require("fs").promises;

// Configuration using GPTC's specific API key from the .env file
const openai = new OpenAI({
	apiKey: process.env.GPTC_API_KEY,
});

// Function to read and parse stock price data from a TSV file
const readStockPriceData = async (filename) => {
	const filePath = `Data/${filename}`;
	try {
		const stockPriceDataText = await fs.readFile(filePath, {
			encoding: "utf8",
		});
		const rows = stockPriceDataText
			.trim()
			.split("\n")
			.map((row) => row.split("\t"));
		const headers = rows.shift(); // Extract headers
		const data = rows.map((row) => {
			return row.reduce((obj, value, index) => {
				obj[headers[index]] = value;
				return obj;
			}, {});
		});
		return data;
	} catch (error) {
		console.error(
			`Error reading stock price data from file: ${filePath}`,
			error
		);
		throw error;
	}
};

// Function to analyze stock prices using GPT model
const analyzeStockPricesWithGPT = async (stockPrices) => {
	const prompt = `Analyze the following stock price data for trends and patterns, and make a concrete prediction for the next trading day. Clearly state whether stock prices are expected to rise or fall, and specify the expected percentage change or price range. Your prediction must be quantitative and actionable, enabling validation against actual market outcomes. Consider historical trends, recent market behaviour, and any notable anomalies in the data. The stock price data to analyze is: ${JSON.stringify(
		stockPrices
	)}`;

	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"You are a financial analyst. Your task is to analyze the provided stock price data and make a concrete prediction about future stock price movements. Your prediction must be clear, quantitative, and actionable.",
				},
				{ role: "user", content: prompt },
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			console.error(
				"Invalid response from the API or missing data:",
				completion
			);
			throw new Error("Invalid response structure from API.");
		}

		const insights = completion.choices[0].message.content.trim();
		return insights;
	} catch (error) {
		console.error(
			"Error during GPT model analysis of stock prices:",
			error
		);
		throw error;
	}
};

// Export functions to be used by the orchestration program
module.exports = {
	readStockPriceData,
	analyzeStockPricesWithGPT,
};


========================================
File: ./src/adaptors/gptd-adaptor.js
========================================

// File: src/adaptors/gptd-adaptor.js

const { OpenAI } = require("openai");
const fs = require("fs").promises;
require("dotenv").config();

// Configuration using GPTD's specific API key from the .env file
const openai = new OpenAI({
	apiKey: process.env.GPTD_API_KEY,
});

// Function to read JSON data from a file
const readJSONData = async (filename) => {
	const filePath = `Data/${filename}`;
	try {
		const dataJson = await fs.readFile(filePath, { encoding: "utf8" });
		return JSON.parse(dataJson);
	} catch (error) {
		console.error(`Error reading JSON file: ${filePath}`, error);
		throw error;
	}
};

// Function to integrate and analyze predictions from GPTB and GPTC
const integrateAndAnalyzePredictions = async (day) => {
	try {
		const nextDay = `day${parseInt(day.replace("day", "")) + 1}`;
		const gptbPredictionData = await readJSONData(
			`log.GPTB.prediction.${nextDay}.json`
		);
		const gptcAnalysisData = await readJSONData(
			`log.GPTC.${day}.analysis.json`
		);

		const prompt = `Integrate and analyze predictions from GPTB and GPTC for Day ${day}, assessing the alignment and discrepancies between the two forecasts. Ensure the analysis highlights key points of agreement and divergence between the models, providing a comprehensive understanding of their predictions. Predictions from GPTB: ${JSON.stringify(
			gptbPredictionData.prediction
		)}, Predictions from GPTC: ${JSON.stringify(gptcAnalysisData)}.`;

		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"Synthesize the information from GPTB and GPTC models to provide a cohesive analysis. Your analysis should integrate insights from both models, highlighting areas of agreement and divergence, and explain the implications for stock price movements. Ensure the analysis is detailed and includes quantitative assessments where possible.",
				},
				{
					role: "user",
					content: prompt,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			throw new Error("Invalid response structure from API.");
		}

		const combinedAnalysis = completion.choices[0].message.content.trim();
		await fs.writeFile(
			`Data/log.GPTD.${day}.analysis.json`,
			JSON.stringify({ combinedAnalysis }, null, 2),
			{ encoding: "utf8" }
		);

		return combinedAnalysis;
	} catch (error) {
		console.error(
			"Error during integration and analysis of predictions:",
			error
		);
		throw error;
	}
};

// Function to make a final prediction for the next trading day stock prices
const makeFinalPrediction = async (day) => {
	try {
		const analysisData = await readJSONData(
			`log.GPTD.${day}.analysis.json`
		);

		const nextDay = `day${parseInt(day.replace("day", "")) + 1}`;
		const prompt = `Based on the integrated analysis from Day ${day}, synthesize insights to make a final, comprehensive prediction for ${nextDay} stock prices. Your prediction should clearly state whether stock prices are expected to rise or fall, by how much, and the reasoning behind your forecast. Ensure the prediction is quantitative, specifying the expected percentage change or price range. Consider historical trends, recent market behaviour, and any notable anomalies in the data. Analysis data: ${JSON.stringify(
			analysisData
		)}.`;

		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"Provide a detailed forecast using the integrated analysis from GPTB and GPTC. Your forecast should clearly state whether stock prices will rise or fall, by how much, and include the reasoning behind your prediction. Ensure the forecast is actionable and precise, enabling validation against actual market outcomes. I repeate, you must state a percentage and if it is going to rise or fall, this is not an option, it is a necessity, you must draw from what you read the conclusion of a rise or fall on prices and by exactly how much.",
				},
				{
					role: "user",
					content: prompt,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			throw new Error("Invalid response structure from API.");
		}

		const finalPrediction = completion.choices[0].message.content.trim();
		await fs.writeFile(
			`Data/log.GPTD.${nextDay}.prediction.json`,
			JSON.stringify({ finalPrediction }, null, 2),
			{ encoding: "utf8" }
		);

		return finalPrediction;
	} catch (error) {
		console.error("Error making final prediction for stock prices:", error);
		throw error;
	}
};

// Export functions to be used by the orchestration program
module.exports = {
	integrateAndAnalyzePredictions,
	makeFinalPrediction,
};


========================================
File: ./src/adaptors/gpta-adaptor.js
========================================

// File: src/adaptors/gpta-adaptor.js

// Import necessary libraries
const { OpenAI } = require("openai");
const fs = require("fs").promises; // Using fs.promises for async file operations
require("dotenv").config();

// Configuration using GPTA's specific API key from the .env file
const openai = new OpenAI({
	apiKey: process.env.GPTA_API_KEY,
});

// Utility function to log data to a file, ensuring it is in JSON format
const logToFile = async (filename, data) => {
	const path = `Data/${filename}`;
	const dataToLog = JSON.stringify(data, null, 2); // Convert data to a JSON string
	await fs.writeFile(path, dataToLog, { encoding: "utf8" }); // Write data as JSON
};

// Function to collect and log news data from a file
const collectAndLogNews = async (day) => {
	const filePath = `Data/news-data-${day}.txt`;

	try {
		const newsDataText = await fs.readFile(filePath, {
			encoding: "utf8",
		});

		// Log the collected news data as JSON
		await logToFile(`log.news.${day}.txt`, { content: newsDataText });

		return newsDataText; // Return text directly
	} catch (error) {
		console.error(`Error reading news data from file for ${day}:`, error);
		throw error;
	}
};

// Function to extract key information from news using GPT-4 with the chat completions endpoint
const extractKeyInformation = async (newsData, day) => {
	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"Please analyse the following text and extract key information that could significantly impact stock market trends. Focus on identifying critical details related to corporate earnings, economic announcements, geopolitical events, market forecasts, and other influential factors. Ensure the summary is concise and highlights the potential market implications of each identified element.",
				},
				{
					role: "user",
					content: newsData,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			console.error(
				"Invalid response from the API or missing data:",
				completion
			);
			throw new Error("Invalid response structure from API.");
		}

		const extractedInformation =
			completion.choices[0].message.content.trim();
		await logToFile(`log.GPTA.${day}.extracted.txt`, {
			extractedInformation,
		});

		return extractedInformation;
	} catch (error) {
		console.error(`Error extracting key information for ${day}:`, error);
		throw error;
	}
};

// Function to execute sentiment analysis on extracted news, now using the chat completions endpoint
const executeSentimentAnalysis = async (extractedInformation, day) => {
	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{
					role: "system",
					content:
						"Utilise the provided information to perform a comprehensive sentiment analysis, determining the overall sentiment (positive, negative, or neutral) and its intensity. Focus on how this sentiment might affect stock market trends, considering the potential impact on market movements, investor behaviour, and future market forecasts. Provide a detailed explanation of your analysis and its implications for stock market trends.",
				},
				{
					role: "user",
					content: extractedInformation,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			console.error(
				"Invalid response from the API or missing data:",
				completion
			);
			throw new Error("Invalid response structure from API.");
		}

		const sentimentAnalysis = completion.choices[0].message.content.trim();
		await logToFile(`log.GPTA.${day}.sentiment.txt`, { sentimentAnalysis });

		return sentimentAnalysis;
	} catch (error) {
		console.error(`Error during sentiment analysis for ${day}:`, error);
		throw error;
	}
};

// Export functions to be used by the orchestration program
module.exports = {
	collectAndLogNews,
	extractKeyInformation,
	executeSentimentAnalysis,
};


========================================
File: ./src/adaptors/gptb-adaptor.js
========================================

// File: src/adaptors/gptb-adaptor.js

// Import necessary libraries
const { OpenAI } = require("openai");
const fs = require("fs").promises;
require("dotenv").config();

// Configuration using GPTB's specific API key from the .env file
const openai = new OpenAI({
	apiKey: process.env.GPTB_API_KEY,
});

// Utility function to read and validate JSON data logged by GPTA
const readAndValidateData = async (filename) => {
	const filePath = `Data/${filename}`;
	try {
		const dataJson = await fs.readFile(filePath, { encoding: "utf8" });
		const data = JSON.parse(dataJson);
		if (!data) {
			throw new Error("Invalid or empty JSON data");
		}
		return data;
	} catch (error) {
		console.error(`Error processing file: ${filePath}`, error);
		throw error; // Ensure errors are propagated up
	}
};

// Function to read and parse stock price data which is not in JSON format
const readStockPriceData = async (filename) => {
	const filePath = `Data/${filename}`;
	try {
		const stockPriceDataText = await fs.readFile(filePath, {
			encoding: "utf8",
		});
		// Assuming the data is tab-separated; adjust if it uses commas or another delimiter
		const rows = stockPriceDataText
			.split("\n")
			.map((row) => row.split("\t"));

		// Optionally convert rows into a more structured format if needed
		const headers = rows[0];
		const data = rows.slice(1).map((row) => {
			let rowData = {};
			row.forEach((value, index) => {
				rowData[headers[index]] = value;
			});
			return rowData;
		});

		return data; // Return parsed data as an array of objects
	} catch (error) {
		console.error(`Error processing stock price file: ${filePath}`, error);
		throw error;
	}
};

// Function to analyze how news, sentiment, and stock prices affected market trends
const analyzeImpactOnStockPrices = async (
	extractedInfo,
	sentimentAnalysis,
	stockPrices,
	day
) => {
	const prompt = `Utilise the following data to conduct a comprehensive analysis of how these factors might influence stock price movements for ${day}. Focus on the extracted information, its sentiment analysis, and the corresponding stock price data. Your analysis should cover the following aspects in detail:

1. **Relevance to Stock Prices**: Identify and explain the direct relevance of the extracted information to stock market trends. Specify the presence of company earnings reports, policy changes, market sentiment shifts, geopolitical events, or other significant factors, and discuss their potential impact on stock prices.

2. **Sentiment Influence**: Analyse how the sentiment (positive, negative, neutral) expressed in the news correlates with observed or potential stock price movements. Consider whether the sentiment could lead to increased trading volume, market optimism or pessimism, or price volatility, and provide examples to support your analysis.

3. **Causative Links**: Establish and explain any causative links between the news sentiment and stock price fluctuations. Highlight clear patterns where certain types of news or sentiment consistently impact stock prices, and discuss the strength and reliability of these patterns.

4. **Comparative Analysis**: Compare the impact of the current day's news and sentiment with data from previous days. Identify cumulative effects or changing trends that might influence future stock price predictions. Discuss how these trends could affect market behaviour and provide insights for future analysis.

5. **Potential Anomalies or Exceptions**: Identify and explain any anomalies or exceptions where the expected impact of news sentiment did not align with actual stock price movements. Suggest possible reasons for these discrepancies, considering factors such as market conditions, external influences, or data inconsistencies.

Provide a detailed and structured analysis, incorporating quantitative and qualitative insights to support your conclusions.`;

	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{ role: "system", content: prompt },
				{
					role: "user",
					content: JSON.stringify({
						extractedInformation: extractedInfo,
						sentiment: sentimentAnalysis,
						stockPrices: stockPrices,
					}),
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			throw new Error("Invalid response structure from API.");
		}

		const impactAnalysis = completion.choices[0].message.content.trim();
		await fs.writeFile(
			`Data/log.GPTB.${day}.impact.json`,
			JSON.stringify({ impactAnalysis }, null, 2),
			{ encoding: "utf8" }
		);
		return impactAnalysis;
	} catch (error) {
		console.error(
			`Error analyzing impact on stock prices for ${day}:`,
			error
		);
		throw error;
	}
};

// Function to predict future stock prices based on the analysis
const predictStockPrices = async (impactAnalysis, day) => {
	const prompt = `Using the analysis of news impact and market sentiment from Day ${
		parseInt(day.replace("day", "")) - 1
	}, forecast the stock prices for Day ${parseInt(
		day.replace("day", "")
	)}. Your prediction should clearly state whether stock prices will rise or fall, by how much, and the reasoning behind your forecast. Ensure that the prediction is quantitative, specifying the expected percentage change or price range. Consider all relevant factors such as market trends, sentiment shifts, historical data, and any anomalies observed. The prediction must be actionable and precise, enabling further validation and fine-tuning.`;

	try {
		const completion = await openai.chat.completions.create({
			model: "gpt-3.5-turbo",
			messages: [
				{ role: "system", content: prompt },
				{
					role: "user",
					content: impactAnalysis,
				},
			],
		});

		if (
			!completion ||
			!completion.choices ||
			completion.choices.length === 0
		) {
			throw new Error("Invalid response structure from API.");
		}

		const prediction = completion.choices[0].message.content.trim();
		await fs.writeFile(
			`Data/log.GPTB.prediction.day${parseInt(
				day.replace("day", "")
			)}.json`,
			JSON.stringify({ prediction }, null, 2),
			{ encoding: "utf8" }
		);
		return prediction;
	} catch (error) {
		console.error(`Error predicting stock prices for Day ${day}:`, error);
		throw error;
	}
};

// Export functions to be used by the orchestration program
module.exports = {
	readAndValidateData,
	readStockPriceData,
	analyzeImpactOnStockPrices,
	predictStockPrices,
};


========================================
File: ./src/news_fetch/__init__.py
========================================

# File: src/news_fetch/__init__.py

========================================
File: ./src/news_fetch/news_fetch.py
========================================

# File: src/news_fetch/news_fetch.py

import requests
import time
import json
from pprint import pprint
from datetime import datetime, timedelta
import os

# Replace with your credentials
username = os.getenv("USERNAME")
password = os.getenv("PASSWORD")
AppID = os.getenv("APP_ID")


def get_auth_header(username, password, appid):
    # Generate the authorization header for making requests to the Aylien API.
    token_response = requests.post(
        'https://api.aylien.com/v1/oauth/token',
        auth=(username, password),
        data={'grant_type': 'password'}
    )
    token_response.raise_for_status()
    token = token_response.json()['access_token']
    headers = {
        'Authorization': f'Bearer {token}',
        'AppId': appid
    }
    return headers


def get_stories(params, headers, max_stories=100):
    # Fetch stories from the Aylien News API using the provided parameters and headers.
    fetched_stories = []
    stories = None

    while stories is None or len(stories) > 0:
        try:
            response = requests.get('https://api.aylien.com/v6/news/stories', params=params, headers=headers,
                                    timeout=30)
            if response.status_code == 200:
                response_json = response.json()
                stories = response_json.get('stories', [])
                fetched_stories.extend(stories)
                if len(fetched_stories) >= max_stories:
                    fetched_stories = fetched_stories[:max_stories]
                    break
                if 'next_page_cursor' in response_json:
                    params['cursor'] = response_json['next_page_cursor']
                else:
                    break
                print(f"Fetched {len(stories)} stories. Total story count so far: {len(fetched_stories)}")
            elif response.status_code == 429:
                print("Rate limit reached. Sleeping for 10 seconds.")
                time.sleep(10)
            elif 500 <= response.status_code <= 599:
                print(f"Server error {response.status_code}. Sleeping for 260 seconds.")
                time.sleep(260)
            else:
                pprint(response.text)
                break
        except requests.exceptions.Timeout:
            print("Request timed out. Retrying...")
            continue
        except Exception as e:
            print(e)
            break
    return fetched_stories


def filter_stories(stories):
    filtered_stories = []
    for story in stories:
        # Keep the basic fields
        filtered_story = {
            "author": story.get("author", {}),
            "body": story.get("body", ""),
            "summary": story.get("summary", {}).get("sentences", []),
            "title": story.get("title", ""),
            "source": {
                "domain": story.get("source", {}).get("domain", ""),
                "home_page_url": story.get("source", {}).get("home_page_url", ""),
                "name": story.get("source", {}).get("name", "")
            }
        }

        # Filter the categories
        filtered_categories = [
            {"id": category.get("id", ""), "label": category.get("label", ""), "score": category.get("score", "")}
            for category in story.get("categories", [])
            if category.get("id") == "ay.impact"
        ]

        # Add categories if the filtered list is not empty
        if filtered_categories:
            filtered_story["categories"] = filtered_categories

        filtered_stories.append(filtered_story)

    return filtered_stories


def fetch_and_save_news_for_day(date, counter):
    headers = get_auth_header(username, password, AppID)
    params = {
        'published_at': f'[{date}T00:00:00Z TO {date}T23:59:59Z]',
        'language': 'en',
        'categories': '{{taxonomy:aylien AND id:(ay.biz.dividend OR ay.lifesoc.disater OR ay.fin.sharehld OR ay.fin.reports OR ay.pol.civilun OR ay.biz.regulat OR ay.impact.ops OR ay.impact.ratings OR ay.biz.bankrupt) AND score: [0.8 TO 1]}}',
        'source.name': '("Yahoo Finance" OR "Marketwatch" OR "Investing" OR "Nasdaq" OR "CNBC" OR "StockMarketWire (UK)" OR "Market Screener" OR "Seeking Alpha" OR "Investors.com" OR "The Motley Fool" OR "INO" OR "Money Control" OR "AlphaStreet" OR "Equitymaster" OR "Washingtonpost.com" OR "New York Times, The" OR "Wall Street Journal")',
        'sentiment.title.polarity': '(negative OR neutral OR positive)',
        'sort_by': 'relevance',
        'per_page': 100
    }

    response = requests.get('https://api.aylien.com/v6/news/stories', params=params, headers=headers)
    response.raise_for_status()
    response_json = response.json()

    stories = response_json.get('stories', [])
    filtered_stories = filter_stories(stories)

    # Ensure the directory exists
    os.makedirs('./News_Data', exist_ok=True)

    # Save the filtered stories to a JSON file
    if filtered_stories:
        filename = f"./News_Data/{counter}_{date.replace('-', '_')}.json"
        with open(filename, "w") as file:
            json.dump(filtered_stories, file, indent=4)
        print(f"Filtered stories saved to {filename}")
    else:
        print(f"No stories found for {date}")


if __name__ == '__main__':
    start_date = datetime.strptime("2023-01-02", "%Y-%m-%d")
    end_date = datetime.strptime("2024-07-14", "%Y-%m-%d")
    current_date = start_date
    counter = 1

    while current_date <= end_date:
        date_str = current_date.strftime("%Y-%m-%d")
        fetch_and_save_news_for_day(date_str, counter)
        current_date += timedelta(days=1)
        counter += 1

========================================
File: ./src/orchestration/orchestration-program.js
========================================

// File: src/orchestration/orchestration-program.js

// Import necessary libraries and adaptors
const {
	collectAndLogNews,
	extractKeyInformation,
	executeSentimentAnalysis,
} = require("./gpta_adaptor");
const {
	readAndValidateData,
	readStockPriceData: readBStockPriceData,
	analyzeImpactOnStockPrices,
	predictStockPrices,
} = require("./gptb_adaptor");
const {
	readStockPriceData: readCStockPriceData,
	analyzeStockPricesWithGPT,
} = require("./gptc_adaptor");
const {
	integrateAndAnalyzePredictions,
	makeFinalPrediction,
} = require("./gptd_adaptor");
const fs = require("fs").promises;
require("dotenv").config();

// Orchestration function to coordinate adaptors
const orchestrateAdaptors = async (day) => {
	try {
		// Step 1: GPTA collects and processes news data
		const newsData = await collectAndLogNews(day);
		const extractedInfo = await extractKeyInformation(newsData, day);
		const sentimentAnalysis = await executeSentimentAnalysis(
			extractedInfo,
			day
		);

		// Log outputs from GPTA for verification and further use
		const logDataGPTA = {
			extractedInformation: extractedInfo,
			sentimentAnalysis: sentimentAnalysis,
		};
		await fs.writeFile(
			`Data/log.GPTA.${day}.output.json`,
			JSON.stringify(logDataGPTA, null, 2),
			{ encoding: "utf8" }
		);

		// Step 2: GPTB reads the data validated by GPTA and analyses impact
		const extractedInfoData = await readAndValidateData(
			`log.GPTA.${day}.output.json`
		);
		const stockPricesDataB = await readBStockPriceData(
			`prices-data-${day}.txt`
		);

		// Analyze the impact on stock prices using GPTB
		const impactAnalysisB = await analyzeImpactOnStockPrices(
			extractedInfoData.extractedInformation,
			extractedInfoData.sentimentAnalysis,
			stockPricesDataB,
			day
		);
		await fs.writeFile(
			`Data/log.GPTB.${day}.impact.json`,
			JSON.stringify({ impactAnalysis: impactAnalysisB }, null, 2),
			{ encoding: "utf8" }
		);

		// Step 3: GPTB makes a prediction based on its analysis
		const predictionDay = `day${parseInt(day.replace("day", "")) + 1}`;
		const predictionB = await predictStockPrices(
			impactAnalysisB,
			predictionDay
		);
		await fs.writeFile(
			`Data/log.GPTB.prediction.${predictionDay}.json`,
			JSON.stringify({ prediction: predictionB }, null, 2),
			{ encoding: "utf8" }
		);

		// Step 4: GPTC reads and analyses the raw stock price data
		const stockPricesDataC = await readCStockPriceData(
			`prices-data-${day}.txt`
		);
		const impactAnalysisC = await analyzeStockPricesWithGPT(
			stockPricesDataC
		);
		await fs.writeFile(
			`Data/log.GPTC.${day}.analysis.json`,
			JSON.stringify({ impactAnalysis: impactAnalysisC }, null, 2),
			{ encoding: "utf8" }
		);

		// Step 5: GPTD integrates and analyzes predictions from GPTB and GPTC, then makes a final prediction
		const integratedAnalysis = await integrateAndAnalyzePredictions(day);
		const finalPrediction = await makeFinalPrediction(day);
		await fs.writeFile(
			`Data/log.GPTD.${predictionDay}.prediction.json`,
			JSON.stringify({ finalPrediction }, null, 2),
			{ encoding: "utf8" }
		);

		console.log(`Orchestration completed for ${day}`);
	} catch (error) {
		console.error("Error during orchestration:", error);
	}
};

// Example usage to orchestrate operations for a specific day
(async () => {
	const day = "day0"; // Specify the day for processing
	await orchestrateAdaptors(day);
})();

module.exports = { orchestrateAdaptors };


========================================
File: ./src/planner/execute.js
========================================

// File: src/planner/execute.js

const { createPlanner } = require(".");

async function runPlanner() {
	try {
		console.log("Starting planner execution...");
		await createPlanner();
		console.log("Planner execution completed successfully.");
	} catch (error) {
		console.error("Error executing planner:", error);
	}
}

// Execute the planner
runPlanner();


========================================
File: ./src/planner/index.js
========================================

// File: src/planner/index.js

const fs = require("fs").promises;
const path = require("path");
require("dotenv").config();

// Define paths to the JSON files using environment variables
const dailyPath = process.env.DAILY_PATH;
const weeklyPath = process.env.WEEKLY_PATH;
const monthlyPath = process.env.MONTHLY_PATH;
const newsPath = process.env.NEWS_PATH;
const plannerPath = process.env.PLANNER_PATH;

async function readJSON(filePath) {
	try {
		console.log(`Reading JSON file from path: ${filePath}`);
		const data = await fs.readFile(filePath, "utf8");
		console.log(`Successfully read JSON file from path: ${filePath}`);
		return JSON.parse(data);
	} catch (error) {
		console.error(`Error reading JSON file from path: ${filePath}`, error);
		throw error;
	}
}

function formatDate(dateStr) {
	const [year, month, day] = dateStr.split("-");
	return `${year}-${month}-${day}`;
}

async function createPlanner() {
	try {
		console.log("Starting to create planner...");

		const dailyData = await readJSON(dailyPath);
		const weeklyData = await readJSON(weeklyPath);
		const monthlyData = await readJSON(monthlyPath);
		const newsData = await readJSON(newsPath);

		const dailySeries = dailyData["Time Series (Daily)"];
		const weeklySeries = weeklyData["Weekly Time Series"];
		const monthlySeries = monthlyData["Monthly Time Series"];

		const planner = [];
		let lastNewsDate = null;

		const newsEntriesArray = Object.entries(newsData);

		for (const [dailyKey, dailyValue] of Object.entries(dailySeries)) {
			console.log(`Processing daily entry: ${dailyKey}`);
			const position = dailyKey.split("_")[0];
			const dailyDate = dailyKey.split("_")[1];
			const formattedDate = formatDate(dailyDate);
			const entry = { position: parseInt(position), daily: dailyKey };

			// Include weekly data if it matches the daily date
			const weeklyKey = Object.keys(weeklySeries).find(
				(key) => key.split("_")[1] === dailyDate
			);
			if (weeklyKey) {
				entry.weekly = weeklyKey;
				console.log(`Including weekly entry: ${weeklyKey}`);
			}

			// Include monthly data if it matches the daily date
			const monthlyKey = Object.keys(monthlySeries).find(
				(key) => key.split("_")[1] === dailyDate
			);
			if (monthlyKey) {
				entry.monthly = monthlyKey;
				console.log(`Including monthly entry: ${monthlyKey}`);
			}

			// Include news data up to the current daily date
			const newsEntries = [];
			for (const [newsKey, newsValue] of newsEntriesArray) {
				const newsDate = newsKey.split("_").slice(1).join("-");
				const formattedNewsDate = formatDate(newsDate);
				if (
					(lastNewsDate === null ||
						new Date(formattedNewsDate) > new Date(lastNewsDate)) &&
					new Date(formattedNewsDate) <= new Date(formattedDate)
				) {
					newsEntries.push(newsKey);
				}
			}
			entry.news = newsEntries.join(", ");
			console.log(`Including news entries: ${entry.news}`);

			// Update lastNewsDate to the latest included news date
			if (newsEntries.length > 0) {
				lastNewsDate = newsEntries[newsEntries.length - 1]
					.split("_")
					.slice(1)
					.join("-");
			}

			planner.push(entry);
		}

		await fs.writeFile(
			plannerPath,
			JSON.stringify(planner, null, 2),
			"utf8"
		);
		console.log("Planner JSON file created successfully");
	} catch (error) {
		console.error("Error creating planner:", error);
		throw error;
	}
}

// Export the createPlanner function
module.exports = { createPlanner };

// Run the createPlanner function
createPlanner().catch((error) => {
	console.error("Error creating planner:", error);
});


========================================
File: ./src/stock_fetch/__init__.py
========================================

# File: src/stock_fetch/__init__.py

========================================
File: ./src/stock_fetch/stock_fetch.py
========================================

# File: src/stock_fetch/stock_fetch.py

import requests
import json
import os
from datetime import datetime, timedelta

# Replace with your Alpha Vantage API key
API_KEY = os.getenv('ALPHA_VANTAGE_API_KEY')
BASE_URL = os.getenv('ALPHA_VANTAGE_BASE_URL')

# Function to fetch data from Alpha Vantage API
def fetch_data(function, symbol='SPY', outputsize='compact', interval=None):
    params = {
        'function': function,
        'symbol': symbol,
        'apikey': API_KEY,
        'outputsize': outputsize
    }
    if interval:
        params['interval'] = interval

    response = requests.get(BASE_URL, params=params)
    data = response.json()
    
    if "Error Message" in data:
        raise ValueError(f"Error fetching data: {data['Error Message']}")
    if "Note" in data:
        raise ValueError(f"Note from API: {data['Note']}")
    if "Information" in data:
        raise ValueError(f"Information from API: {data['Information']}")
    return data

# Function to filter data by date range
def filter_data_by_date(data, start_date, end_date, key):
    if key not in data:
        raise ValueError(f"Key '{key}' not found in data")
    
    filtered_data = {key: {}}
    start_date = datetime.strptime(start_date, '%Y-%m-%d')
    end_date = datetime.strptime(end_date, '%Y-%m-%d')

    for date_str, values in data[key].items():
        date = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S' if ' ' in date_str else '%Y-%m-%d')
        if start_date <= date < end_date:  # Changed <= to < for end_date consistency
            filtered_data[key][date_str] = values
    
    return filtered_data

# Function to save data to a JSON file
def save_data(data, filename):
    os.makedirs('stock_data', exist_ok=True)
    filepath = os.path.join('stock_data', filename)
    with open(filepath, 'w') as file:
        json.dump(data, file, indent=4)
    print(f"Data saved to {filepath}")

# Fetch and save daily data
def fetch_and_save_daily(symbol='SPY', start_date='2023-01-01', end_date='2024-03-11'):
    data = fetch_data('TIME_SERIES_DAILY', symbol, outputsize='full')
    print(f"Fetched daily data: {list(data.keys())}")  # Debug statement
    filtered_data = filter_data_by_date(data, start_date, end_date, 'Time Series (Daily)')
    print(f"Filtered data for range {start_date} to {end_date}: {json.dumps(filtered_data, indent=4)[:500]}")  # Debug statement
    save_data(filtered_data, f'daily_{symbol}.json')

# Fetch and save weekly data
def fetch_and_save_weekly(symbol='SPY', start_date='2023-01-02', end_date='2024-07-14'):
    data = fetch_data('TIME_SERIES_WEEKLY', symbol, outputsize='full')
    print(f"Fetched weekly data: {list(data.keys())}")  # Debug statement
    filtered_data = filter_data_by_date(data, start_date, end_date, 'Weekly Time Series')
    save_data(filtered_data, f'weekly_{symbol}.json')

# Fetch and save monthly data
def fetch_and_save_monthly(symbol='SPY', start_date='2023-01-02', end_date='2024-07-14'):
    data = fetch_data('TIME_SERIES_MONTHLY', symbol, outputsize='full')
    print(f"Fetched monthly data: {list(data.keys())}")  # Debug statement
    filtered_data = filter_data_by_date(data, start_date, end_date, 'Monthly Time Series')
    save_data(filtered_data, f'monthly_{symbol}.json')

if __name__ == '__main__':
    symbol = 'SPY'  # S&P 500 ETF as a proxy
    start_date = '2023-01-02'
    end_date = '2024-07-14'
    try:
        fetch_and_save_daily(symbol, start_date, end_date)
        fetch_and_save_weekly(symbol, start_date, end_date)
        fetch_and_save_monthly(symbol, start_date, end_date)
        print("Data fetching completed successfully.")
    except ValueError as e:
        print(e)
    except Exception as e:
        print(f"An error occurred: {e}")

========================================
File: ./scripts/combine_code_script.py
========================================

import os

# Define the directories and file extensions to include
directories_to_include = [
    "./scripts",
    "./src/adaptors",
    "./src/news_fetch",
    "./src/orchestration",
    "./src/planner",
    "./src/stock_fetch"
]
extensions_to_include = ['.py', '.js']

# Define the output file path
output_file_path = './docs/combined_code.txt'

# Start writing to the output file
with open(output_file_path, 'w') as output_file:
    for directory in directories_to_include:
        for root, _, files in os.walk(directory):
            for file in files:
                if any(file.endswith(ext) for ext in extensions_to_include):
                    file_path = os.path.join(root, file)
                    output_file.write(f"\n{'='*40}\n")
                    output_file.write(f"File: {file_path}\n")
                    output_file.write(f"{'='*40}\n\n")
                    with open(file_path, 'r') as f:
                        output_file.write(f.read())
                        output_file.write("\n")

    # Include this script's code at the end of the file
    script_path = './scripts/combine_code_script.py'
    output_file.write(f"\n{'='*40}\n")
    output_file.write(f"File: {script_path}\n")
    output_file.write(f"{'='*40}\n\n")
    with open(__file__, 'r') as f:
        output_file.write(f.read())
        output_file.write("\n")

print(f"All code has been combined into {output_file_path}")
